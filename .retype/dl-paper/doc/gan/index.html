<!DOCTYPE html>
<html lang="en" class="h-full">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="turbo-cache-control" content="no-cache" data-turbo-track="reload" data-track-token="2.3.0.715268998224">

    <!-- See retype.com -->
    <meta name="generator" content="Retype 2.3.0">

    <!-- Primary Meta Tags -->
    <title>Generative Models</title>
    <meta name="title" content="Generative Models">
    <meta name="description" content="GAN과 VAE 비교" />

    <!-- Canonical -->
    <link rel="canonical" href="https://tech/dl-paper/doc/gan/" />

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://tech/dl-paper/doc/gan/">
    <meta property="og:title" content="Generative Models">
    <meta property="og:description" content="GAN과 VAE 비교">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://tech/dl-paper/doc/gan/">
    <meta property="twitter:title" content="Generative Models">
    <meta property="twitter:description" content="GAN과 VAE 비교">

    <script>(function () { var el = document.documentElement, m = localStorage.getItem("doc_theme"), wm = window.matchMedia; if (m === "dark" || (!m && wm && wm("(prefers-color-scheme: dark)").matches)) { el.classList.add("dark") } else { el.classList.remove("dark") } })();</script>

    <link href="../../../resources/css/retype.css?v=2.3.0.715268998224" rel="stylesheet" />

    <script type="text/javascript" src="../../../resources/js/config.js?v=2.3.0.715268998224" data-turbo-eval="false" defer></script>
    <script type="text/javascript" src="../../../resources/js/retype.js?v=2.3.0" data-turbo-eval="false" defer></script>
    <script id="lunr-js" type="text/javascript" src="../../../resources/js/lunr.js?v=2.3.0.715268998224" data-turbo-eval="false" defer></script>
</head>
<body>
    <div id="docs-app" class="relative text-base antialiased text-gray-700 bg-white font-body dark:bg-dark-850 dark:text-dark-300">
        <div class="absolute bottom-0 left-0 bg-gray-100 dark:bg-dark-800" style="top: 5rem; right: 50%"></div>
    
        <header id="docs-site-header" class="sticky top-0 z-30 flex w-full h-16 bg-white border-b border-gray-200 md:h-20 dark:bg-dark-850 dark:border-dark-650">
            <div class="container relative flex items-center justify-between grow pr-6 md:justify-start">
                <!-- Mobile menu button skeleton -->
                <button v-cloak class="skeleton docs-mobile-menu-button flex items-center justify-center shrink-0 overflow-hidden dark:text-white focus:outline-none rounded-full w-10 h-10 ml-3.5 md:hidden"><svg xmlns="http://www.w3.org/2000/svg" class="mb-px shrink-0" width="24" height="24" viewBox="0 0 24 24" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor"><path d="M2 4h20v2H2zM2 11h20v2H2zM2 18h20v2H2z"></path></g></svg></button>
                <div v-cloak id="docs-sidebar-toggle"></div>
        
                <!-- Logo -->
                <div class="flex items-center justify-between h-full py-2 md:w-75">
                    <div class="flex items-center px-2 md:px-6">
                        <a id="docs-site-logo" href="../../../" class="flex items-center leading-snug text-2xl">
                            <span class="w-10 mr-2 grow-0 shrink-0 overflow-hidden">
                                <img class="max-h-10 dark:hidden md:inline-block" src="../../../static/saintly-logo.svg">
                                <img class="max-h-10 hidden dark:inline-block" src="../../../static/saintly-logo-dark.svg">
                            </span>
                            <span class="dark:text-white font-semibold line-clamp-1 md:line-clamp-2">S.KYUHWN</span>
                        </a>
                    </div>
        
                    <span class="hidden h-8 border-r md:inline-block dark:border-dark-650"></span>
                </div>
        
                <div class="flex justify-between md:grow">
                    <!-- Top Nav -->
                    <nav class="hidden md:flex">
                        <ul class="flex flex-col mb-4 md:pl-16 md:mb-0 md:flex-row md:items-center">
                            <li class="mr-6">
                                <a class="py-2 md:mb-0 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="../../../">Home</a>
                            </li>
                            <li class="mr-6">
                                <a class="py-2 md:mb-0 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://github.com/underthelights/">GitHub</a>
                            </li>
                            <li class="mr-6">
                                <a class="py-2 md:mb-0 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://drive.google.com/file/d/1lcEg4fXJvTQlCKW5mWai_ej8NZGg21yT/view">Resume</a>
                            </li>
                            <li class="mr-6">
                                <a class="py-2 md:mb-0 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://www.linkedin.com/in/kyuhwan-shim/">Linkedin</a>
                            </li>
                            <li class="mr-6">
                                <a class="py-2 md:mb-0 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://underthelights.github.io">GitBlog</a>
                            </li>
        
                        </ul>
                    </nav>
        
                    <!-- Header Right Skeleton -->
                    <div v-cloak class="flex justify-end grow skeleton">
        
                        <!-- Search input mock -->
                        <div class="relative hidden w-40 lg:block lg:max-w-sm lg:ml-auto">
                            <div class="absolute flex items-center justify-center h-full pl-3 dark:text-dark-300">
                                <svg xmlns="http://www.w3.org/2000/svg" class="icon-base" width="16" height="16" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation"  style="margin-bottom: 1px;"><g fill="currentColor" ><path d="M21.71 20.29l-3.68-3.68A8.963 8.963 0 0020 11c0-4.96-4.04-9-9-9s-9 4.04-9 9 4.04 9 9 9c2.12 0 4.07-.74 5.61-1.97l3.68 3.68c.2.19.45.29.71.29s.51-.1.71-.29c.39-.39.39-1.03 0-1.42zM4 11c0-3.86 3.14-7 7-7s7 3.14 7 7c0 1.92-.78 3.66-2.04 4.93-.01.01-.02.01-.02.01-.01.01-.01.01-.01.02A6.98 6.98 0 0111 18c-3.86 0-7-3.14-7-7z" ></path></g></svg>
                            </div>
        
                            <input class="w-full h-10 transition-colors duration-200 ease-in bg-gray-200 border border-transparent rounded md:text-sm hover:bg-white hover:border-gray-300 focus:outline-none focus:bg-white focus:border-gray-500 dark:bg-dark-600 dark:border-dark-600 placeholder-gray-400 dark:placeholder-dark-400"
                            style="padding: 0.625rem 0.75rem 0.625rem 2rem" type="text" placeholder="Search" />
                        </div>
        
                        <!-- Mobile search button mock -->
                        <div class="flex items-center justify-center w-10 h-10 lg:hidden">
                            <svg xmlns="http://www.w3.org/2000/svg" class="shrink-0 icon-base" width="20" height="20" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation"  style="margin-bottom: 0px;"><g fill="currentColor" ><path d="M21.71 20.29l-3.68-3.68A8.963 8.963 0 0020 11c0-4.96-4.04-9-9-9s-9 4.04-9 9 4.04 9 9 9c2.12 0 4.07-.74 5.61-1.97l3.68 3.68c.2.19.45.29.71.29s.51-.1.71-.29c.39-.39.39-1.03 0-1.42zM4 11c0-3.86 3.14-7 7-7s7 3.14 7 7c0 1.92-.78 3.66-2.04 4.93-.01.01-.02.01-.02.01-.01.01-.01.01-.01.02A6.98 6.98 0 0111 18c-3.86 0-7-3.14-7-7z" ></path></g></svg>
                        </div>
        
                        <!-- Dark mode switch placehokder -->
                        <div class="w-10 h-10 lg:ml-2"></div>
        
                        <!-- History button mock -->
                        <div class="flex items-center justify-center w-10 h-10" style="margin-right: -0.625rem;">
                            <svg xmlns="http://www.w3.org/2000/svg" class="shrink-0 icon-base" width="22" height="22" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation"  style="margin-bottom: 0px;"><g fill="currentColor" ><g ><path d="M12.01 6.01c-.55 0-1 .45-1 1V12a1 1 0 00.4.8l3 2.22a.985.985 0 001.39-.2.996.996 0 00-.21-1.4l-2.6-1.92V7.01c.02-.55-.43-1-.98-1z"></path><path d="M12.01 1.91c-5.33 0-9.69 4.16-10.05 9.4l-.29-.26a.997.997 0 10-1.34 1.48l1.97 1.79c.19.17.43.26.67.26s.48-.09.67-.26l1.97-1.79a.997.997 0 10-1.34-1.48l-.31.28c.34-4.14 3.82-7.41 8.05-7.41 4.46 0 8.08 3.63 8.08 8.09s-3.63 8.08-8.08 8.08c-2.18 0-4.22-.85-5.75-2.4a.996.996 0 10-1.42 1.4 10.02 10.02 0 007.17 2.99c5.56 0 10.08-4.52 10.08-10.08.01-5.56-4.52-10.09-10.08-10.09z"></path></g></g></svg>
                        </div>
                    </div>
        
                    <div v-cloak class="flex justify-end grow">
                        <div id="docs-mobile-search-button"></div>
                        <doc-search-desktop></doc-search-desktop>
        
                        <doc-theme-switch class="lg:ml-2"></doc-theme-switch>
                        <doc-history></doc-history>
                    </div>
                </div>
            </div>
        </header>
    
        <div class="container relative flex bg-white">
            <!-- Sidebar Skeleton -->
            <div v-cloak class="fixed flex flex-col shrink-0 duration-300 ease-in-out bg-gray-100 border-gray-200 sidebar top-20 w-75 border-r h-screen md:sticky transition-transform skeleton dark:bg-dark-800 dark:border-dark-650">
            
                <!-- Render this div, if config.showSidebarFilter is `true` -->
                <div class="flex items-center h-16 px-6">
                    <input class="w-full h-8 px-3 py-2 transition-colors duration-200 ease-linear bg-white border border-gray-200 rounded shadow-none text-sm focus:outline-none focus:border-gray-600 dark:bg-dark-600 dark:border-dark-600" type="text" placeholder="Filter" />
                </div>
            
                <div class="pl-6 mb-4 mt-1">
                    <div class="w-32 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-48 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-40 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-32 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-48 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-40 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                </div>
            
                <div class="shrink-0 mt-auto bg-transparent dark:border-dark-650">
                    <a class="flex items-center justify-center flex-nowrap h-16 text-gray-400 dark:text-dark-400 hover:text-gray-700 dark:hover:text-dark-300 transition-colors duration-150 ease-in docs-powered-by" target="_blank" href="https://retype.com/" rel="noopener">
                        <span class="text-xs whitespace-nowrap">Powered by</span>
                        <svg xmlns="http://www.w3.org/2000/svg" class="ml-2" fill="currentColor" width="96" height="20" overflow="visible"><path d="M0 0v20h13.59V0H0zm11.15 17.54H2.44V2.46h8.71v15.08zM15.8 20h2.44V4.67L15.8 2.22zM20.45 6.89V20h2.44V9.34z"/><g><path d="M40.16 8.44c0 1.49-.59 2.45-1.75 2.88l2.34 3.32h-2.53l-2.04-2.96h-1.43v2.96h-2.06V5.36h3.5c1.43 0 2.46.24 3.07.73s.9 1.27.9 2.35zm-2.48 1.1c.26-.23.38-.59.38-1.09 0-.5-.13-.84-.4-1.03s-.73-.28-1.39-.28h-1.54v2.75h1.5c.72 0 1.2-.12 1.45-.35zM51.56 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92h4.74v1.83h-6.79V5.36h6.64zM60.09 7.15v7.48h-2.06V7.15h-2.61V5.36h7.28v1.79h-2.61zM70.81 14.64h-2.06v-3.66l-3.19-5.61h2.23l1.99 3.45 1.99-3.45H74l-3.19 5.61v3.66zM83.99 6.19c.65.55.97 1.4.97 2.55s-.33 1.98-1 2.51-1.68.8-3.04.8h-1.23v2.59h-2.06V5.36h3.26c1.42 0 2.45.28 3.1.83zm-1.51 3.65c.25-.28.37-.69.37-1.22s-.16-.92-.48-1.14c-.32-.23-.82-.34-1.5-.34H79.7v3.12h1.38c.68 0 1.15-.14 1.4-.42zM95.85 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92H96v1.83h-6.79V5.36h6.64z"/></g></svg>
                    </a>
                </div>
            </div>
            
            <!-- Sidebar component -->
            <doc-sidebar v-cloak>
                <template #sidebar-footer>
                    <div class="shrink-0 mt-auto border-t md:bg-transparent md:border-none dark:border-dark-650">
            
                        <div class="py-3 px-6 md:hidden border-b dark:border-dark-650">
                            <nav>
                                <ul class="flex flex-wrap justify-center items-center">
                                    <li class="mr-6">
                                        <a class="block py-1 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="../../../">Home</a>
                                    </li>
                                    <li class="mr-6">
                                        <a class="block py-1 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://github.com/underthelights/">GitHub</a>
                                    </li>
                                    <li class="mr-6">
                                        <a class="block py-1 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://drive.google.com/file/d/1lcEg4fXJvTQlCKW5mWai_ej8NZGg21yT/view">Resume</a>
                                    </li>
                                    <li class="mr-6">
                                        <a class="block py-1 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://www.linkedin.com/in/kyuhwan-shim/">Linkedin</a>
                                    </li>
                                    <li class="mr-6">
                                        <a class="block py-1 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://underthelights.github.io">GitBlog</a>
                                    </li>
            
                                </ul>
                            </nav>
                        </div>
            
                        <a class="flex items-center justify-center flex-nowrap h-16 text-gray-400 dark:text-dark-400 hover:text-gray-700 dark:hover:text-dark-300 transition-colors duration-150 ease-in docs-powered-by" target="_blank" href="https://retype.com/" rel="noopener">
                            <span class="text-xs whitespace-nowrap">Powered by</span>
                            <svg xmlns="http://www.w3.org/2000/svg" class="ml-2" fill="currentColor" width="96" height="20" overflow="visible"><path d="M0 0v20h13.59V0H0zm11.15 17.54H2.44V2.46h8.71v15.08zM15.8 20h2.44V4.67L15.8 2.22zM20.45 6.89V20h2.44V9.34z"/><g><path d="M40.16 8.44c0 1.49-.59 2.45-1.75 2.88l2.34 3.32h-2.53l-2.04-2.96h-1.43v2.96h-2.06V5.36h3.5c1.43 0 2.46.24 3.07.73s.9 1.27.9 2.35zm-2.48 1.1c.26-.23.38-.59.38-1.09 0-.5-.13-.84-.4-1.03s-.73-.28-1.39-.28h-1.54v2.75h1.5c.72 0 1.2-.12 1.45-.35zM51.56 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92h4.74v1.83h-6.79V5.36h6.64zM60.09 7.15v7.48h-2.06V7.15h-2.61V5.36h7.28v1.79h-2.61zM70.81 14.64h-2.06v-3.66l-3.19-5.61h2.23l1.99 3.45 1.99-3.45H74l-3.19 5.61v3.66zM83.99 6.19c.65.55.97 1.4.97 2.55s-.33 1.98-1 2.51-1.68.8-3.04.8h-1.23v2.59h-2.06V5.36h3.26c1.42 0 2.45.28 3.1.83zm-1.51 3.65c.25-.28.37-.69.37-1.22s-.16-.92-.48-1.14c-.32-.23-.82-.34-1.5-.34H79.7v3.12h1.38c.68 0 1.15-.14 1.4-.42zM95.85 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92H96v1.83h-6.79V5.36h6.64z"/></g></svg>
                        </a>
                    </div>
                </template>
            </doc-sidebar>
    
            <div class="grow min-w-0 dark:bg-dark-850">
                <!-- Render "toolbar" template here on api pages --><!-- Render page content -->
                <div class="flex">
                    <div class="grow min-w-0 px-6 md:px-16">
                        <main class="relative pt-6 pb-16">
                            <div class="docs-markdown" id="docs-content">
                                <!-- Rendered if sidebar right is enabled -->
                                <div id="docs-sidebar-right-toggle"></div>
                
                                <!-- Page content  -->
<doc-anchor-target id="generative-models" class="break-words">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#generative-models">#</doc-anchor-trigger>
        <span>Generative Models</span>
    </h1>
</doc-anchor-target>
<doc-anchor-target id="개론">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#개론">#</doc-anchor-trigger>
        <span>개론</span>
    </h2>
</doc-anchor-target>
<ul>
<li>GAN과 VAE 비교
<ul>
<li><a href="https://taeoh-kim.github.io/blog/generative-models-part-1-vaegandcgan/">https://taeoh-kim.github.io/blog/generative-models-part-1-vaegandcgan/</a></li>
<li><a href="https://www.quora.com/What-are-the-pros-and-cons-of-Generative-Adversarial-Networks-vs-Variational-Autoencoders">https://www.quora.com/What-are-the-pros-and-cons-of-Generative-Adversarial-Networks-vs-Variational-Autoencoders</a></li>
</ul>
</li>
</ul>
<doc-anchor-target id="gan">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#gan">#</doc-anchor-trigger>
        <span>GAN</span>
    </h2>
</doc-anchor-target>
<p>최윤제님 정리 자료</p>
<ul>
<li>원본: <a href="https://www.youtube.com/watch?v=uQT464Ms6y8">https://www.youtube.com/watch?v=uQT464Ms6y8</a></li>
<li>네이버 버전: <a href="https://www.youtube.com/watch?v=odpjk7_tGY0">https://www.youtube.com/watch?v=odpjk7_tGY0</a></li>
<li>슬라이드: <a href="https://www.slideshare.net/NaverEngineering/1-gangenerative-adversarial-network">https://www.slideshare.net/NaverEngineering/1-gangenerative-adversarial-network</a></li>
</ul>
<p>AtoZ:</p>
<ul>
<li><a href="http://nbviewer.jupyter.org/github/metamath1/ml-simple-works/blob/master/GAN/GANs.ipynb">http://nbviewer.jupyter.org/github/metamath1/ml-simple-works/blob/master/GAN/GANs.ipynb</a></li>
</ul>
<p>GAN tutorial</p>
<ul>
<li>2016 한글 정리: <a href="https://kakalabblog.wordpress.com/2017/07/27/gan-tutorial-2016/">https://kakalabblog.wordpress.com/2017/07/27/gan-tutorial-2016/</a></li>
<li>2017: <a href="https://nips.cc/Conferences/2016/Schedule?showEvent=6202">https://nips.cc/Conferences/2016/Schedule?showEvent=6202</a></li>
</ul>
<p>블로그</p>
<ul>
<li>컨셉과 원리: <a href="http://learnai.tistory.com/">http://learnai.tistory.com/</a></li>
<li>구현: <a href="http://jaynewho.com/post/2">http://jaynewho.com/post/2</a></li>
<li>라온피플 자료: <a href="http://laonple.blog.me/221190581073">http://laonple.blog.me/221190581073</a></li>
</ul>
<doc-anchor-target id="dcgan">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#dcgan">#</doc-anchor-trigger>
        <span>DCGAN</span>
    </h2>
</doc-anchor-target>
<p>DCGAN 논문 리뷰 한글:</p>
<ul>
<li><a href="http://laonple.blog.me/221201915691">http://laonple.blog.me/221201915691</a></li>
<li><a href="http://artoria.us/19">http://artoria.us/19</a></li>
<li><a href="https://jaejunyoo.blogspot.com/2017/02/deep-convolutional-gan-dcgan-1.html">https://jaejunyoo.blogspot.com/2017/02/deep-convolutional-gan-dcgan-1.html</a></li>
<li>소개 동영상: <a href="https://www.youtube.com/watch?v=7btUjE2y4NA">https://www.youtube.com/watch?v=7btUjE2y4NA</a></li>
<li><a href="https://kakalabblog.wordpress.com/2017/06/04/unsupervised-representation-learning-with-dcgan-2016-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">https://kakalabblog.wordpress.com/2017/06/04/unsupervised-representation-learning-with-dcgan-2016-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/</a></li>
<li>독창적인 소개: <a href="https://dev-strender.github.io/articles/2017-07/decan-introduction">https://dev-strender.github.io/articles/2017-07/decan-introduction</a></li>
</ul>
<p>참조하기 좋은 자료들</p>
<ul>
<li><a href="http://www.khshim.com/archives/218">http://www.khshim.com/archives/218</a></li>
<li><a href="https://kakalabblog.wordpress.com/2017/06/10/gandcgan-%EB%A6%AC%EB%B7%B0-%EB%B0%9C%ED%91%9C/">https://kakalabblog.wordpress.com/2017/06/10/gandcgan-%EB%A6%AC%EB%B7%B0-%EB%B0%9C%ED%91%9C/</a></li>
</ul>
<doc-anchor-target id="cgan">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#cgan">#</doc-anchor-trigger>
        <span>cGAN</span>
    </h2>
</doc-anchor-target>
<doc-anchor-target id="introduction">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#introduction">#</doc-anchor-trigger>
        <span>Introduction</span>
    </h3>
</doc-anchor-target>
<ul>
<li>GAN
<ul>
<li>주목받는 이유
<ul>
<li>까다로운 확률 계산을 approximating하는 것은 어려운데, 이것을 회피할 수 있는 generative model 학습 framework가 대안으로 뜨고 있다.</li>
</ul>
</li>
<li>장점
<ul>
<li>Markov chain도 필요없고</li>
<li>gradient를 얻기 위해 backprop만 사용되고</li>
<li>학습 과정에서 inference도 필요없고</li>
<li>다양한 factor와 interation을 모델에 쉽게 포함시킬 수 있고</li>
<li>S.O.T.A log-likelihood estimate와 진짜같은 sample을 만들 수 있다.</li>
</ul>
</li>
</ul>
</li>
<li>cGAN의 특징
<ul>
<li>기존 GAN과는 달리 추가적인 정보를 사용하여 data generation process를 제어하는 것이 가능함</li>
<li>이러한 conditioning은 아래와 같은 것들이 될 수 있다.
<ul>
<li>class label</li>
<li>some part of data for inpainting (<a href="https://www.slideshare.net/PulkitGoyal1/image-inpainting">inpainting</a>, <a href="https://papers.nips.cc/paper/5024-multi-prediction-deep-boltzmann-machines.pdf">참고논문 Fig. 3</a>)</li>
<li>data from different modality</li>
</ul>
</li>
</ul>
</li>
<li>이 논문에서는 두 가지 데이터셋으로 실험을 진행했다.
<ul>
<li>MNIST: condition이 class label</li>
<li>MIR Flickr: condition이 multi-modal 정보</li>
</ul>
</li>
</ul>
<doc-anchor-target id="related-work">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#related-work">#</doc-anchor-trigger>
        <span>Related Work</span>
    </h3>
</doc-anchor-target>
<p>기존 supervised neural networks의 2가지 challenge</p>
<ol>
<li>아주 많은 수의 output category를 예측하는 모델로의 확장이 어렵다.
<ul>
<li>해결책: 다른 modality의 정보를 활용한다. (<a href="https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/41869.pdf">참고논문 Fig. 1</a>)</li>
<li>=&gt; prediction error라도 ground truth에 가까울 수 있고(e.g. table 대신 chair를 예측),</li>
<li>=&gt; 학습 과정에서 unseen인 label에까지 generalized prediction이 가능하다</li>
</ul>
</li>
<li>input에서 output으로의 매핑이 one-to-one mapping을 학습하는데 많은 연구가 집중되어 있지만, 많은 실제 문제들은 probabilistic one-to-many mapping이다. (e.g. 이미지 하나에 대해 다양한 태깅이 존재할 수 있음)
<ul>
<li>해결책: conditional probabilistic generative model을 사용한다.</li>
<li>=&gt; multi-modal Deep Boltzmann Machine을 학습하는 방법 (cGAN 방법론과 유사한 접근법)</li>
<li>=&gt; multi-modal neural language model 학습해서 이미지에 대한 다양한(= one-to-many) description을 생성</li>
</ul>
</li>
</ol>
<doc-anchor-target id="conditional-adversarial-nets">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#conditional-adversarial-nets">#</doc-anchor-trigger>
        <span>Conditional Adversarial Nets</span>
    </h3>
</doc-anchor-target>
<p>generator와 discriminator에 추가적인 정보 y로 condition을 주면 GAN을 conditional model로 확장할 수 있다.</p>
<ul>
<li>Generator: input noise p(z)와 y가 joint hidden representation으로 결합된다. (참고로 adversarial training framework는 hidden representation을 구성하는 방식에 상당한 유연성을 제공한다. 반면 전통적인 generative framework에서는 이게 엄청 어려웠음)</li>
<li>Discriminator: x와 y가 입력으로 discriminator function에 제공된다.</li>
<li>Objective function:<br />
![alt text]<a href="../images/gan/cgan_loss2.png" title="cgan_loss2.png">image1</a></li>
<li>Structure:<br />
![alt text]<a href="../images/gan/cgan_structure.png" title="cgan_structure.png">image2</a></li>
</ul>
<doc-anchor-target id="experimental-results">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#experimental-results">#</doc-anchor-trigger>
        <span>Experimental Results</span>
    </h3>
</doc-anchor-target>
<doc-anchor-target id="1-unimodal-mnist">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#1-unimodal-mnist">#</doc-anchor-trigger>
        <span>1) Unimodal (MNIST)</span>
    </h4>
</doc-anchor-target>
<ul>
<li>conditional adversarial net on MNIST images:
<ul>
<li>(one-hot vector로 인코딩된) class labels로 conditioned된 MNIST 이미지로 conditional adversarial net을 학습함.</li>
</ul>
</li>
<li>Generator 구현부 설명
<ul>
<li>100차원 unit hypercube에서 uniform distribution으로부터 z 추출함.</li>
<li>z와 y는 각각 size 200, 1000 짜리 hidden layer(w/ ReLU)로 매핑됨. 그러고나서 양쪽 모두 size 1200짜리 두 번째 combined hidden ReLU 레이어로 매핑됨.</li>
<li>마지막으로 784 차원의 MNIST 샘플을 output으로 생성하는 sigmoid unit layer가 있음.</li>
<li><img src="../images/gan/cgan_generator.png" alt="alt text" title="cgan_generator.png" /></li>
</ul>
</li>
<li>Discriminator 구현부 설명
<ul>
<li>x, y를 maxout layer(각각 240/5, 50/5)로 매핑한다.</li>
<li>두 hidden layer는 joint maxout layer(240/4)로 매핑된 후 sigmoid layer로 들어간다.</li>
<li>discriminator의 아키텍쳐는 충분한 capacity만 있다면 별로 중요하지 않고, maxout이 이 task에 궁합이 좋다.</li>
<li><img src="../images/gan/cgan_discriminator.png" alt="alt text" title="cgan_discriminator.png" /></li>
<li>mini-batch size: 100</li>
<li>learning rate: 0.1 -&gt; 0.000001</li>
<li>momentum: 0.5 -&gt; 0.7</li>
<li>dropout keepprob: 0.5</li>
<li>stopping point: best estimate of log-likelihood on the validation set</li>
</ul>
</li>
<li>성능 평가 결과
<ul>
<li><img src="../images/gan/cgan_mnist.png" alt="alt text" title="cgan_mnist.png" /></li>
<li>Gaussian Parzen window log-likelihood estimate로 성능 평가</li>
<li>Parzen window distribution을 사용해서 test set의 log-likelihood를 추정했다.(<a href="https://github.com/bt22dr/deep-learning-papers/blob/master/paper/Generative%20Adversarial%20Networks.pdf">참고 논문 5장</a>)
<ul>
<li>막상 결과를 수치로 따져보면 다른 모델이 좀 더 낫다. 근데 우리 모델은 그냥 PoC라서 그런것이고 좀 더 고도화하면 나아질 것이다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<doc-anchor-target id="2-multimodal-flickr">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#2-multimodal-flickr">#</doc-anchor-trigger>
        <span>2) Multimodal (Flickr)</span>
    </h4>
</doc-anchor-target>
<ul>
<li>UGM(user-generated metadata)
<ul>
<li>Flickr같은 사진 사이트에는 labeled data가 많다. (e.g. 사진에 연관된 사용자 태그들)</li>
<li>UGM은 canonical image labelling schems랑은 많이 달라서 좀 더 descriptive하다</li>
<li>UGM은 사람마다 다른 단어를 이용해서 같은 컨셉을 기술하기 때문에 동의어가 많다. 고로 이런 labels를 잘 normalize하는 방법이 중요하다. =&gt; word embedding을 사용한다.</li>
</ul>
</li>
<li>자동화된 이미지 태깅
<ul>
<li>이미지 feature로 condition된 tag-vector의 분포를 생성하기 위해 conditional adversarial nets를 사용</li>
<li>feature
<ul>
<li>image representation: ImageNet으로 pre-train된 network에서 마지막 FC layer의 4096 차원 output을 image representation으로 사용</li>
<li>word representation: YFCC100M으로 skip-gram model을 학습하여 word representation으로 사용</li>
</ul>
</li>
<li>샘플 생성 방법:
<ol>
<li>image feature vector를 condition으로 사용하여 word feature vector를 생성한다.</li>
<li>해당 word representation과 cosine 유사도가 높은 word들을 선별</li>
<li>개중에 10개의 most common words를 선택해서 evaluation에 사용함.</li>
</ol>
</li>
</ul>
</li>
<li>자세한 내용은 논문을 참조</li>
</ul>
<doc-anchor-target id="연관된-연구들-참고-이미지">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#연관된-연구들-참고-이미지">#</doc-anchor-trigger>
        <span>연관된 연구들 (<a href="https://github.com/bt22dr/deep-learning-papers/blob/master/paper/Conditional%20Generative%20Adversarial%20Nets.pdf">참고 이미지</a>)</span>
    </h3>
</doc-anchor-target>
<p>cGAN은 아래 연구들로 발전된다.</p>
<ul>
<li>generative adversarial text to image synthesis<br />
![alt text]<a href="../images/gan/cgan_txt2img.png" title="cgan_txt2img.png">image6</a></li>
<li>pix2pix: Image-to-Image Translation with Conditional Adversarial Networks<br />
![alt text]<a href="../images/gan/cgan_pix2pix.png" title="cgan_pix2pix.png">image7</a></li>
</ul>
<doc-anchor-target id="같이-보면-좋은-자료">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#같이-보면-좋은-자료">#</doc-anchor-trigger>
        <span>같이 보면 좋은 자료</span>
    </h3>
</doc-anchor-target>
<ul>
<li>김승일 소장님 발표: <a href="https://www.youtube.com/watch?v=iCgT8G4PkqI">https://www.youtube.com/watch?v=iCgT8G4PkqI</a></li>
<li>cgan 정리 블로그:
<ul>
<li><a href="http://t-lab.tistory.com/29">http://t-lab.tistory.com/29</a></li>
<li><a href="https://kangbk0120.github.io/articles/2017-08/conditional-gan">https://kangbk0120.github.io/articles/2017-08/conditional-gan</a></li>
</ul>
</li>
</ul>
<doc-anchor-target id="infogan">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#infogan">#</doc-anchor-trigger>
        <span>InfoGAN</span>
    </h2>
</doc-anchor-target>
<ul>
<li>참고자료
<ul>
<li>ppt: <a href="https://www.slideshare.net/ssuser2a5d00/infogan-paper-review">https://www.slideshare.net/ssuser2a5d00/infogan-paper-review</a></li>
<li>code <a href="https://github.com/1202kbs/InfoGAN-Tensorflow">https://github.com/1202kbs/InfoGAN-Tensorflow</a></li>
</ul>
</li>
</ul>
<doc-anchor-target id="auto-encoding-variational-bayes">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#auto-encoding-variational-bayes">#</doc-anchor-trigger>
        <span>Auto-Encoding Variational Bayes</span>
    </h2>
</doc-anchor-target>
<ul>
<li><p>문제제기: continuous latent variables을 가진 intractable posterior distribution과 large datasets가 있을 때 어떻게 directed probabilistic model에서 효율적인 inference and learning을 수행할 수 있을까?</p>
</li>
<li><p>제안하는것: stochastic variational inference and learning algorithm</p>
<ul>
<li>large datasets로 확장 가능</li>
<li>mild differentiability conditions하에서</li>
<li>intractable case에서도 잘 동작</li>
</ul>
</li>
<li><p>contribution:</p>
<ol>
<li>variational lower bound를 reparameterization하여 standard SGD로 최적화할 수 있는 lower bound estimator를 산출한다.</li>
<li>datapoint마다 continuous latent variable이 있는 i.i.d. dataset에 대해, lower bound estimator를 사용해서 approximate inference model(= recognition model)을 intractable posterior에 fitting함으로써 posterior inference를 효율적으로 만들 수 있다.</li>
</ol>
</li>
<li><p>graphical model, expectation maximization, variational inference,  (TODO: 내용 추가)</p>
</li>
</ul>
<doc-anchor-target id="1-introduction">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#1-introduction">#</doc-anchor-trigger>
        <span>1 Introduction</span>
    </h3>
</doc-anchor-target>
<ul>
<li>어떻게 하면 directed probabilistic models에 대한 효율적인 approximate inference and learning을 수행할 수 있을까? (이 모델의 continuous latent variables와 parameters는 intractable posterior distributions를 가진다) (TODO: 가우시안 믹스쳐 그림 추가 (잠재변수, 그래피컬 모델))
<ul>
<li>Variational Bayesian approach: intractable posterior에 대한 approximation을 최적화함</li>
<li>안타깝지만 common mean-field 접근법은 approximate posterior에 대한 기대값의 analytical solution을 필요로 하며, 보통은 이것도 intractable한 경우가 많다. (TODO: PRML에서 closed form이 아닌것들 보여주는 부분 설명 추가)</li>
</ul>
</li>
<li>variational lower bound의 reparameterization이 어떻게 lower bound의 미분가능한 unbiased estimator를 산출하는지 보인다.
<ul>
<li>이 SGVB(Stochastic Gradient Variational Bayes) estimator는 continuous latent variables and/or parameters를 가진 거의 모든 모델에서 효율적인 approximate posterior inference를 위해 사용될 수 있다.</li>
<li>이 estimator는 standard stochastic gradient ascent 테크닉을 사용해서 간단하게 최적화할 수 있다.</li>
</ul>
</li>
<li>AEVB(Auto-Encoding Variational Bayes) 알고리즘을 제안한다.
<ul>
<li>i.i.d. dataset이며 datapoint마다 continuous latent variable 가진 데이터셋에 적용</li>
<li>recognition model을 최적화하기 위해 SGVB estimator를 사용함으로써 AEVB는 inference and learning을 효율적으로 만든다.
<ul>
<li>recognition model은
<ul>
<li>간단한 ancestral sampling을 사용하여 approximate posterior inference를 효율적으로 수행하게 해준다. (ancestral sampling: 그래피컬 모델이 주어졌을 때 부모 노드에서 자식노드로 이동하는 형태로 샘플링해서 얻은 결합확률로부터 하나의 샘플 (x_1, x_2, ..., x_K)를 얻는 방법)</li>
<li>고로 MCMC 같은 값비싼 iterative inference 기법을 적용하지 않고도 효과적으로 모델 parameter를 학습할 수 있게 해준다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>학습된 approximate posterior inference model은 recognition, denoising, representation, visualization 같은 다수의 task에 사용될 수 있다.</li>
<li>recognition model에 neural network를 사용한게 바로 VAE이다.</li>
</ul>
<doc-anchor-target id="2-method">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#2-method">#</doc-anchor-trigger>
        <span>2 Method</span>
    </h3>
</doc-anchor-target>
<ul>
<li><p>여기서 사용하는 전략은 continuous latent variables를 가진 다양한 directed graphical model에서 lower bound estimator(stochastic objective function)를 유도하는데 사용될 수 있다.</p>
</li>
<li><p>다음과 같은 common case로 제한한다.</p>
<ul>
<li>i.i.d. dataset with latent variables per datapoint</li>
<li>parameter에 대해 maximum likelihood(ML) or maximum a posteriori(MAP) inference 수행</li>
<li>latent variables에 대해 variational inference 수행</li>
</ul>
</li>
<li><p>이 방법론은 streaming data같은 online, non-stationary setting에 적용될 수 있지만 이 논문에서는 단순함을 위해 fixed dataset을 가정한다.</p>
</li>
</ul>
<doc-anchor-target id="21-problem-scenario">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#21-problem-scenario">#</doc-anchor-trigger>
        <span>2.1 Problem scenario</span>
    </h4>
</doc-anchor-target>
<ul>
<li>문제: marginal or posterior probability에 대한 간략화 가정을 하지 않는다. 역으로 아래 경우에도 효율적으로 동작하는 일반적인 알고리즘에 관심이 있다.
<ul>
<li>Intractability: 아래와 같은 intractability들은 꽤 흔하며, 복잡한 likelihood function <img src="http://latex.codecogs.com/svg.latex?p_%5Ctheta%28%5Cmathbf%7Bx%7D%7C%5Cmathbf%7Bz%7D%29" alt="p_\theta(\mathbf|\mathbf)" />의 경우에 나타난다. (e.g. a neural network with a nonlinear hidden layer)
<ul>
<li>marginal likelihood의 적분 <img src="http://latex.codecogs.com/svg.latex?p_%5Ctheta%28%5Cmathbf%7Bx%7D%29%20%3D%20%5Csmallint%20p_%5Ctheta%28%5Cmathbf%7Bz%7D%29p_%5Ctheta%28%5Cmathbf%7Bx%7D%7C%5Cmathbf%7Bz%7D%29%20d%5Cmathbf%7Bz%7D" alt="p_\theta(\mathbf) = \smallint p_\theta(\mathbf)p_\theta(\mathbf|\mathbf) d\mathbf" />가 intractable: marginal likelihood를 미분하거나 계산할 수 없다.</li>
<li>true posterior density <img src="http://latex.codecogs.com/svg.latex?p_%5Ctheta%28%5Cmathbf%7Bz%7D%7C%5Cmathbf%7Bx%7D%29%20%3D%20p_%5Ctheta%28%5Cmathbf%7Bx%7D%7C%5Cmathbf%7Bz%7D%29p_%5Ctheta%28%5Cmathbf%7Bz%7D%29%20/%20p_%5Ctheta%28%5Cmathbf%7Bx%7D%29" alt="" />가 intractable: EM 알고리즘을 사용할 수 없다.</li>
<li>mean-field VB 알고리즘을 위해 필요한 적분이 intractable</li>
</ul>
</li>
<li>A large dataset: 데이터가 많아서 batch optimization이 너무 비싼 경우 minibatch나 single datapoint로 파라미터를 업데이트 하고싶다.
<ul>
<li>Monte Carlo EM같은 샘플링 기반 솔루션은 datapoint 마다 expensive sampling loop가 필요하므로 너무 느리다.</li>
</ul>
</li>
</ul>
</li>
<li>솔루션:
<ul>
<li>Efficient approximate ML or MAP estimation for the parameters theta: 파라미터 자체로 관심의 대상이 될 수 있다. 실제 데이터를 닮은 artificial data 생성 같은 일들을 가능하게 해준다. -&gt; p(x|z)</li>
<li>Efficient approximate posterior inference of the latent variable z given an observed value x for a choice of parameters θ: coding이나 data representation task에 유용하다. -&gt; p(z|x)</li>
<li>Efficient approximate marginal inference of the variable x: x에 대한 prior가 필요한 모든 종류의 inference task를 가능하게 해준다. 이미지 denoising, inpainting, super-resolution 같은 것들. -&gt; p(x)</li>
</ul>
</li>
<li>위 문제들을 해결하기 위해 recognition model <img src="http://latex.codecogs.com/svg.latex?q_%5Cphi%28%5Cmathbf%7Bz%7D%7C%5Cmathbf%7Bx%7D%29" alt="q_\phi(\mathbf|\mathbf)" />를 도입해보자.
<ul>
<li>얘는 intractable true posterior <img src="http://latex.codecogs.com/svg.latex?p_%5Ctheta%28%5Cmathbf%7Bz%7D%7C%5Cmathbf%7Bx%7D%29" alt="p_\theta(\mathbf|\mathbf)" />의 approximation이다.</li>
<li>mean-field variational inference에서 posterior를 approixmate하는 것과는 반대로, 이것은 factorial일 필요도 없고 parameter phi가 closed-form expectation으로부터 계산되지도 않는다.</li>
<li>대신에 recognition model parameter <img src="http://latex.codecogs.com/svg.latex?%5Cphi" alt="" />를 generative model parameter <img src="http://latex.codecogs.com/svg.latex?%5Ctheta" alt="" />와 jointly 학습하는 방법을 소개할 것이다.</li>
</ul>
</li>
<li>coding theory 관점에서 unobserved variables z는 latent representation 또는 code로 생각할 수 있다. 그러므로...
<ul>
<li>recognition model <img src="http://latex.codecogs.com/svg.latex?q_%5Cphi%28%5Cmathbf%7Bz%7D%7C%5Cmathbf%7Bx%7D%29" alt="q_\phi(\mathbf|\mathbf)" />을 encoder로 표기: datapoint x가 주어졌을 때, datapoint x가 생성되어졌을 수 있는 z의 가능한 값에 대한 distribution을 생성하기 때문</li>
<li><img src="http://latex.codecogs.com/svg.latex?p_%5Ctheta%28%5Cmathbf%7Bx%7D%7C%5Cmathbf%7Bz%7D%29" alt="p_\theta(\mathbf|\mathbf)" />를 decoder로 표기: code z가 주어졌을 때, 그에 해당하는 x의 가능한 값에 대한 distribution을 생성하기 때문</li>
</ul>
</li>
</ul>
<doc-anchor-target id="중간정리">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#중간정리">#</doc-anchor-trigger>
        <span>중간정리</span>
    </h4>
</doc-anchor-target>
<ul>
<li>그러니까 정리해보면, 우리가 궁극적으로 구하려고 하는 것은...
<ul>
<li>observed variable x는 unknown underlying process로 부터의 random sample이라고 가정한다. 이 underlying process의 true distribution <img src="http://latex.codecogs.com/svg.latex?p%5E%7B*%7D%28%5Cmathbf%7Bx%7D%29" alt="p^{*}(\mathbf)" />은 unkown이기 때문에 우리는 이 underlying process를 모델 <img src="http://latex.codecogs.com/svg.latex?p_%5Ctheta%28%5Cmathbf%7Bx%7D%29" alt="p_\theta(\mathbf)" />로 approximate 하려는 것이다.</li>
<li><img src="http://latex.codecogs.com/svg.latex?p_%5Ctheta%28%5Cmathbf%7Bx%7D%29%20%5Capprox%20p%5E%7B*%7D%28%5Cmathbf%7Bx%7D%29" alt="p_\theta(\mathbf) \approx  p^{*}(\mathbf)" /> 이러한 parameter theta의 값을 찾는 과정이 바로 learning이다.</li>
</ul>
</li>
<li>만약 fully observed model 상황이라면?
<ul>
<li>directed graphical model의 모든 variable들이 데이터에서 observed 된다면 그냥 straightforward optimazation하면 된다.</li>
<li>보통은 maximum likelihood 방법을 사용하며, maximum likelihood ciriterion은 <img src="http://latex.codecogs.com/svg.latex?log%5C%20p_%5Ctheta%28%5Cmathcal%7BD%7D%29%20%3D%20%5Csum_%7B%20%7D_%7B%5Cmathbf%7Bx%7D%5Cin%20%5Cmathcal%7BD%7D%7D%5E%7B%20%7D%5C%20log%5C%20p_%5Ctheta%28%5Cmathbf%7Bx%7D%29" alt="log\ p_\theta(\mathcal) = \sum_{\mathbf\in \mathcal}^\ log\ p\theta(\mathbf)" /> 이다. 이 objective의 gradient를 구해서 hill-climb 방식으로 iterative하게 local optimum을 찾아간다.
<ul>
<li>all datapoints: batch gradient descent</li>
<li>minibatche data: stochastic gradient descent</li>
</ul>
</li>
</ul>
</li>
<li>latent variable model 상황이라면?
<ul>
<li>latent variable은 모델에서 사용되는 variable이지만 dataset에서는 관측할 수는 없기 때문에 latent variable이라고 말한다.</li>
<li>latent variable을 사용하는 이해하기 쉬운 사례로는 <a href="http://norman3.github.io/prml/docs/chapter09/2">가우시안 혼합 모델</a>이 있음.
<ul>
<li>원래 식 (9.7)이 있지만 잠재변수를 사용하면 복잡한 혼합 모델 p(x) 대신 비교적 간단한 p(x,z)의 marginalization <img src="http://latex.codecogs.com/svg.latex?%5Csum_%7B%20%7D_%7Bz%7D%5C%20p%28%5Cmathbf%7Bx%7D%2C%5Cmathbf%7Bz%7D%29" alt="\sum__\ p(\mathbf,\mathbf)" />으로 p(x)를 나타낼 수 있다. 이러면 조건부 확률을 사용할 수 있으므로 EM 알고리즘도 사용 가능하게 된다.</li>
<li>샘플을 생성하기 위해 우선 p(z) 분포에서 z 값을 생성하고 차례로 ancestral sampling을 수행한다. 이렇게 joint distribution p(x,z) = p(x|z)p(z)를 사용해 생성한 샘플 예제는 그림 (a)에서 확인할 수 있다.</li>
<li>사족으로, 해당 문서 아래쪽에서 단순히 log-likelihood function 미분해서 파라미터를 구할 수 없는 경우(closed-form이 아니기 때문)를 봐두면 나중에 참고가 된다.</li>
</ul>
</li>
<li>latent variable을 도입하면 이 directed graphical model은 joint distribution p_theta(x,z)</li>
</ul>
</li>
<li>그러면 위 식을 이용해서 maximum likelihood를 계산하면 될까?
<ul>
<li><img src="http://latex.codecogs.com/svg.latex?%5Cunderset%7B%5C%20%7D%7Barg%7D%5C%20%5Cunderset%7B%5Ctheta%7D%7Bmax%7D%5Bp_%5Ctheta%28x%29%20%3D%20%5Cint_%7Bz%7D%20p_%5Ctheta%28x%2Cz%29%20%3D%20%5Cint_%7Bz%7D%20p_%5Ctheta%28x%7Cz%29p_%5Ctheta%28z%29%5D" alt="\underset{\ }\ \underset{\theta}[p_\theta(x) = \int_ p_\theta(x,z) = \int_ p_\theta(x|z)p_\theta(z)]" /></li>
<li>하지만 모두 다 intractable해서 계산할 수가 없다.</li>
</ul>
</li>
<li>그래서 parametric inference model q_\phi(z|x) 도입
<ul>
<li>variational inference : General family of methods for approximating complicated densities by a simpler class of densities</li>
<li>variational parameter phi가 <img src="http://latex.codecogs.com/svg.latex?q_%5Cphi%28z%7Cx%29%20%5Capprox%20p_%5Ctheta%28z%7Cx%29" alt="q_\phi(z|x) \approx p_\theta(z|x)" /> 요렇게 되도록 optimize한다.</li>
<li>distribution q_\phi(z|x)를 neural network로 parameterize하면 아래와 같이 표현할 수 있다.
<ul>
<li><img src="http://latex.codecogs.com/svg.latex?%5Cbegin%7Bmatrix%7D%20%28%5Cmu%2C%20log%5C%20%5Csigma%29%20%3D%20EncoderNeuralNet_%5Cphi%28%5Cmathbf%7Bx%7D%29%20%5C%5C%20q_%5Cphi%28%5Cmathbf%7Bz%7D%7C%5Cmathbf%7Bx%7D%29%20%3D%20%5Cmathcal%7BN%7D%28%5Cmathbf%7Bz%7D%3B%5Cmu%2C%20diag%28%5Csigma%29%29%20%5Cend%7Bmatrix%7D" alt="\begin (\mu, log\ \sigma) = EncoderNeuralNet_\phi(\mathbf) \ q_\phi(\mathbf|\mathbf) = \mathcal(\mathbf;\mu, diag(\sigma)) \end" /></li>
</ul>
</li>
</ul>
</li>
<li>여기까지의 의식의 흐름은 다음과 같다. 궁극적으로 알고 싶은건 p*(x)를 근사하는 pθ(x)를 최대화하는 θ를 찾고 싶은건데 p(x)를 바로 알기는 어려우니 z 도입. 즉 pθ(x,z)를 알고 싶다는 얘기. p(x,z)는 prior*decoder 즉 p(z)p(x|z)인데 아무 z~p(z)나 넣으면 샘플 생성이 잘 안되더라. 그래서 샘플 생성을 잘 하는 z를 p(z|x)로 구하면 어떨까? 이게 인코더. 인코더에서 posterior pθ(z|x)를 구해야 하는데 이게 intractable이라서 이걸 qφ(z|x)로 approximate한다.<br />
![alt text]<a href="../images/gan/vae_fig_01.png" title="vae_fig_01.png">image_vae101</a></li>
</ul>
<doc-anchor-target id="22-the-variational-bound">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#22-the-variational-bound">#</doc-anchor-trigger>
        <span>2.2 The variational bound</span>
    </h4>
</doc-anchor-target>
<ul>
<li>marginal likelihood는 개별 datapoint들의 marginal likelihood의 합으로 구성된다. 즉 <img src="http://latex.codecogs.com/svg.latex?log%5C%20p_%5Ctheta%28%5Cmathbf%7Bx%7D%5E%7B%281%29%7D%2C%20...%20%2C%20%5Cmathbf%7Bx%7D%5E%7B%28N%29%7D%29%20%3D%20%5Csum_%7B%20%7D_%7Bi%3D1%7D%5E%7BN%7D%5C%20log%5C%20p_%5Ctheta%28%5Cmathbf%7Bx%7D%5E%7B%28i%29%7D%29" alt="log\ p_\theta(\mathbf{(1)}, ... , \mathbf{(N)}) = \sum_^\ log\ p\theta(\mathbf^{(i)})" />이며, 각 likelihood 식은 아래와 같이 전개된다.</li>
<li><img src="../images/gan/vae_eq_01.png" alt="alt text" title="vae_eq_01.png" />
<ul>
<li>수식 전개는 <a href="https://www.slideshare.net/NaverEngineering/ss-96581209">활석님 자료</a>의 VAE 7 page 참고</li>
<li>first RHS term: approximate와 true posterior의 KL divergence</li>
<li>second RHS term: datapoint i의 marginal likelihood에 대한 (variational) lower bound라고도 하며, 베이지안에서 marginal likelihood를 model evidence라고 하므로 evidence lower bound(ELBO)라고도 한다.</li>
</ul>
</li>
<li>고로 식 (1)은 아래와 같이 다시 쓸 수 있는데</li>
<li><img src="../images/gan/vae_eq_02.png" alt="alt text" title="vae_eq_02.png" />
<ul>
<li>이렇게 하는 이유는 앞에서 봤듯이 pθ(x)에 직접 maximum likelihood를 적용할 수 없기 때문에 ELBO를 최대화 할것이다. L을 최대화하면 maximum likelihood도 최대화되겠지.</li>
<li>수식 전개는 <a href="https://www.youtube.com/watch?v=KYA-GEhObIs">pr12-vae</a>의 ELBO - eq(2) 부분 참고</li>
</ul>
</li>
<li>식 (2)를 다시 쓰면 아래와 같다.</li>
<li><img src="../images/gan/vae_eq_03.png" alt="alt text" title="vae_eq_03.png" />
<ul>
<li>수식 전개는 <a href="https://www.slideshare.net/NaverEngineering/ss-96581209">활석님 자료</a>의 VAE 8 page 참고</li>
</ul>
</li>
<li>ELBO를 최대화하는 것은 2 가지를 측면을 동시에 optimize한다.
<ul>
<li>pθ(x) 최대화: generative model이 점점 더 좋아진다.</li>
<li>KL divergence 최소화: qφ(z|x)를 우리가 잘 다룰 수 있는 tractable한 분포(ex. gaussian) pθ(z)와 비슷하게 만든다.</li>
<li><a href="http://sanghyukchun.github.io/70/">Decomposition of log-likelihood 참고</a>참고</li>
</ul>
</li>
<li>이제 ELBO를 두 파라미터(variational parameter φ와 generative parameter θ)에 대해 미분하고 최적화해보자. (<a href="https://pure.uva.nl/ws/files/17891313/Thesis.pdf">kingma thesis</a> 18 page 참고)
<ul>
<li>∇θELBO: simple.</li>
<li>∇φELBO: problematic. 이런 문제를 위한 monte carlo gradient estimator는 샘플링 때문에 high variance가 나타나며 impractical하다. -&gt; reparameterization trick이 필요하다.</li>
</ul>
</li>
</ul>
<doc-anchor-target id="23-the-sgvb-estimator-and-aevb-algorithm-잘-모름">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#23-the-sgvb-estimator-and-aevb-algorithm-잘-모름">#</doc-anchor-trigger>
        <span>2.3 The SGVB estimator and AEVB algorithm (잘 모름)</span>
    </h4>
</doc-anchor-target>
<p>여기서는 approximate posterior가 qφ(z|x) 형태인데 위에서 살펴본 monte carlo gradient estimator는 x로 conditioned되지 않는 qφ(z)의 경우만에 적용될 수 있다는 점에 유의하라.</p>
<ul>
<li>특정한 mild condition을 가정하면 approximate posterior qφ(z|x)를 위해서 미분 가능한 transformation gφ(e, x)를 사용하여 random variable <img src="http://latex.codecogs.com/svg.latex?%5Ctilde%7B%5Cmathbf%7Bz%7D%7D%20%5Csim%20q_%5Cphi%28%5Cmathbf%7Bz%7D%7C%5Cmathbf%7Bx%7D%29" alt="\tilde{\mathbf} \sim  q_\phi(\mathbf|\mathbf)" />를 reparameterizate할 수 있다. 여기서 e는 (auxiliary) noise variable.</li>
<li>수식으로 나타내면 다음과 같다.</li>
<li><img src="../images/gan/vae_eq_04.png" alt="alt text" title="vae_eq_04.png" /></li>
<li>이렇게 하면 이제 어떤 function f(z)의 expectation의 qφ(z|x)에 대한 monte carlo estimate를 아래와 같이 수행할 수 있다.</li>
<li><img src="../images/gan/vae_eq_05.png" alt="alt text" title="vae_eq_05.png" /></li>
<li>이 테크닉을 수식 (2)에 적용하면 아래와 같이 generic Stochastic Gradient Variational Bayes(SBVB) estimator를 얻을 수 있다.</li>
<li><img src="../images/gan/vae_eq_06.png" alt="alt text" title="vae_eq_06.png" /></li>
<li>이 테크닉을 수식 (3)에 적용하면 KL-divergence term 말고 expected reconstruction error term만 sampling으로 estimation하면 된다. 이때 KL-divergence term은 φ를 regularizing한다고 해석할 수 있다. 즉, approximate posterior qφ(z|x)를 pθ(z)에 가깝게 만든다.</li>
<li>여기서 식 (3)에 해당하는 두 번째 버전의 SGVB estimator가 아래와 같이 나온다. 보통은 이게 앞에서 본 generic estimator 보다 작은 variance를 갖는다.</li>
<li><img src="../images/gan/vae_eq_07.png" alt="alt text" title="vae_eq_07.png" /></li>
<li>정리하면... <a href="https://www.slideshare.net/NaverEngineering/ss-96581209">활석님 자료</a>의 VAE 11 page 참고</li>
</ul>
<doc-anchor-target id="24-the-reparameterization-trick">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#24-the-reparameterization-trick">#</doc-anchor-trigger>
        <span>2.4 The reparameterization trick</span>
    </h4>
</doc-anchor-target>
<ul>
<li>continuous latent variable과 미분가능하 encoder와 generative model이 있을 때, change of variables를 통해 ELBO는 θ와 φ에 대해 straightforward하게 미분가능해질 수 있다.</li>
<li><a href="https://pure.uva.nl/ws/files/17891313/Thesis.pdf">kingma thesis</a> Figure 2.3 참고</li>
</ul>
<doc-anchor-target id="함께-보면-좋은-자료">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#함께-보면-좋은-자료">#</doc-anchor-trigger>
        <span>함께 보면 좋은 자료</span>
    </h3>
</doc-anchor-target>
<ul>
<li>VAE 논문: <a href="https://arxiv.org/pdf/1312.6114.pdf">https://arxiv.org/pdf/1312.6114.pdf</a></li>
<li>킹마형 Thesis: <a href="https://pure.uva.nl/ws/files/17891313/Thesis.pdf">https://pure.uva.nl/ws/files/17891313/Thesis.pdf</a></li>
<li>VAE tutorial: <a href="https://arxiv.org/abs/1606.05908">https://arxiv.org/abs/1606.05908</a></li>
<li>직관이 좋은 슬라이드 자료: <a href="https://www.slideshare.net/ssuser06e0c5/variational-autoencoder-76552518">https://www.slideshare.net/ssuser06e0c5/variational-autoencoder-76552518</a></li>
<li>동전 던지기로 설명: <a href="http://www.openias.org/variational-coin-toss">http://www.openias.org/variational-coin-toss</a></li>
<li>EM 관련: <a href="https://www.youtube.com/watch?v=MBr47eM4hf0">https://www.youtube.com/watch?v=MBr47eM4hf0</a></li>
<li>GMM 관련: <a href="https://www.youtube.com/watch?v=JNlEIEwe-Cg">https://www.youtube.com/watch?v=JNlEIEwe-Cg</a></li>
<li>Bloomberg (EM for Latent): <a href="https://www.youtube.com/watch?v=lMShR1vjbUo">https://www.youtube.com/watch?v=lMShR1vjbUo</a></li>
<li>CS231N (Generative Models): <a href="https://www.youtube.com/watch?v=5WoItGTWV54">https://www.youtube.com/watch?v=5WoItGTWV54</a></li>
</ul>
<doc-anchor-target id="wavegan">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#wavegan">#</doc-anchor-trigger>
        <span>WaveGAN</span>
    </h2>
</doc-anchor-target>
<ul>
<li>PR12 발표 동영상: <a href="https://www.youtube.com/watch?v=UXVKSSXdwb8">https://www.youtube.com/watch?v=UXVKSSXdwb8</a></li>
<li>구현: <a href="https://github.com/chrisdonahue/wavegan/">https://github.com/chrisdonahue/wavegan/</a></li>
</ul>
<doc-anchor-target id="wgan">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#wgan">#</doc-anchor-trigger>
        <span>WGAN</span>
    </h2>
</doc-anchor-target>
<ul>
<li><p>Wasserstein distance</p>
<ul>
<li>wgan 수학 이해하기: <a href="https://www.slideshare.net/ssuser7e10e4/wasserstein-gan-i">https://www.slideshare.net/ssuser7e10e4/wasserstein-gan-i</a></li>
<li>gan to wgan: <a href="https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html">https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html</a></li>
<li><a href="http://dogfoottech.tistory.com/185">http://dogfoottech.tistory.com/185</a></li>
<li>wasserstein 거리: <a href="https://rosinality.github.io/2017/04/wasserstein-%EA%B1%B0%EB%A6%AC/">https://rosinality.github.io/2017/04/wasserstein-%EA%B1%B0%EB%A6%AC/</a></li>
</ul>
</li>
<li><p>Lipschitz Continuity</p>
<ul>
<li>연속성의 종류: <a href="http://mathnmath.tistory.com/42">http://mathnmath.tistory.com/42</a></li>
<li>갑자기 왜 Lipschitz continuity가 나온걸까? convex optimization 때문: <a href="http://sanghyukchun.github.io/63/">http://sanghyukchun.github.io/63/</a></li>
</ul>
</li>
<li><p>Kantorovich-Rubinstein Duality</p>
<ul>
<li><a href="https://vincentherrmann.github.io/blog/wasserstein/">https://vincentherrmann.github.io/blog/wasserstein/</a></li>
</ul>
</li>
</ul>
<doc-anchor-target id="sngan">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#sngan">#</doc-anchor-trigger>
        <span>SNGAN</span>
    </h2>
</doc-anchor-target>
<p>논문 설명</p>
<ul>
<li><a href="http://jaejunyoo.blogspot.com/2018/05/paper-skim-spectral-normalization-for-gan.html">http://jaejunyoo.blogspot.com/2018/05/paper-skim-spectral-normalization-for-gan.html</a></li>
<li><a href="http://keunwoochoi.blogspot.com/2018/01/spectral-normalization-for-generative.html">http://keunwoochoi.blogspot.com/2018/01/spectral-normalization-for-generative.html</a></li>
<li><a href="https://www.youtube.com/watch?v=iXSYqohGQhM">https://www.youtube.com/watch?v=iXSYqohGQhM</a></li>
</ul>
<p>matrix norm 관련</p>
<ul>
<li>wikipedia: <a href="https://en.wikipedia.org/wiki/Matrix_norm#Special_Cases">https://en.wikipedia.org/wiki/Matrix_norm#Special_Cases</a></li>
<li><a href="https://www.math.uh.edu/%7Ejingqiu/math4364/iterative_linear_system.pdf">https://www.math.uh.edu/~jingqiu/math4364/iterative_linear_system.pdf</a></li>
<li><a href="https://www.youtube.com/watch?v=UtNPmGw60jg">https://www.youtube.com/watch?v=UtNPmGw60jg</a></li>
<li><a href="https://www.youtube.com/watch?v=WPMY_ufrmzI">https://www.youtube.com/watch?v=WPMY_ufrmzI</a></li>
<li><a href="https://www.youtube.com/watch?v=pgJ2Sg1jcYQ">https://www.youtube.com/watch?v=pgJ2Sg1jcYQ</a></li>
</ul>
<p>Lipschitz 관련</p>
<ul>
<li><a href="http://sanghyukchun.github.io/63/">http://sanghyukchun.github.io/63/</a></li>
<li><a href="http://users.ece.utexas.edu/%7Ecmcaram/EE381V_2012F/Lecture_4_Scribe_Notes.final.pdf">http://users.ece.utexas.edu/~cmcaram/EE381V_2012F/Lecture_4_Scribe_Notes.final.pdf</a></li>
<li>동영상
<ul>
<li><a href="https://www.youtube.com/watch?v=Cnc83B3C2pY">https://www.youtube.com/watch?v=Cnc83B3C2pY</a></li>
<li><a href="https://www.youtube.com/watch?v=_MKhnZ_n3PY">https://www.youtube.com/watch?v=_MKhnZ_n3PY</a></li>
<li><a href="https://www.youtube.com/watch?v=yzRDBw1bSV0">https://www.youtube.com/watch?v=yzRDBw1bSV0</a></li>
<li><a href="https://www.youtube.com/watch?v=sv1R8gualO8">https://www.youtube.com/watch?v=sv1R8gualO8</a></li>
<li><a href="https://www.youtube.com/watch?v=5VK5-KfYXDI">https://www.youtube.com/watch?v=5VK5-KfYXDI</a></li>
<li><a href="https://www.youtube.com/watch?v=GMiOPeqAwlw">https://www.youtube.com/watch?v=GMiOPeqAwlw</a></li>
<li><a href="https://www.youtube.com/watch?v=VpYyGfs0SdA">https://www.youtube.com/watch?v=VpYyGfs0SdA</a></li>
<li><a href="https://www.youtube.com/watch?v=pI9faLfYrH8">https://www.youtube.com/watch?v=pI9faLfYrH8</a></li>
</ul>
</li>
</ul>
<doc-anchor-target id="cs236">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#cs236">#</doc-anchor-trigger>
        <span>CS236</span>
    </h1>
</doc-anchor-target>
<doc-anchor-target id="1-autoregressive-models">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#1-autoregressive-models">#</doc-anchor-trigger>
        <span>1. Autoregressive Models</span>
    </h2>
</doc-anchor-target>
<ul>
<li>NADE: The Neural Autoregressive Distribution Estimator (<a href="http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf">http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf</a>)</li>
<li>RNADE: The real-valued neural autoregressive density-estimator (<a href="https://arxiv.org/abs/1306.0186">https://arxiv.org/abs/1306.0186</a>)</li>
<li>MADE: Masked Autoencoder for Distribution Estimation (<a href="https://arxiv.org/abs/1502.03509">https://arxiv.org/abs/1502.03509</a>)</li>
<li>Pixel Recurrent Neural Networks (<a href="https://arxiv.org/abs/1601.06759">https://arxiv.org/abs/1601.06759</a>)</li>
<li>Conditional Image Generation with PixelCNN Decoders (<a href="https://arxiv.org/abs/1606.05328">https://arxiv.org/abs/1606.05328</a>)</li>
<li>PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications (<a href="https://arxiv.org/abs/1701.05517">https://arxiv.org/abs/1701.05517</a>)</li>
<li>WaveNet: A Generative Model for Raw Audio (<a href="https://arxiv.org/abs/1609.03499">https://arxiv.org/abs/1609.03499</a>)</li>
</ul>
<doc-anchor-target id="2-variational-autoencoders">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#2-variational-autoencoders">#</doc-anchor-trigger>
        <span>2. Variational Autoencoders</span>
    </h2>
</doc-anchor-target>
<doc-anchor-target id="3-normalizing-flow-models">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#3-normalizing-flow-models">#</doc-anchor-trigger>
        <span>3. Normalizing Flow Models</span>
    </h2>
</doc-anchor-target>
<ul>
<li>Real NVP 저자 직강: <a href="https://www.youtube.com/watch?v=7hKul_tOfsI">https://www.youtube.com/watch?v=7hKul_tOfsI</a></li>
<li>수학
<ul>
<li>Change-of-Variable Technique: <a href="https://newonlinecourses.science.psu.edu/stat414/node/157/">https://newonlinecourses.science.psu.edu/stat414/node/157/</a></li>
<li>Jacobian determinant: <a href="http://www.math.ucla.edu/%7Earchristian/teaching/32b-sum18/session-4.pdf">http://www.math.ucla.edu/~archristian/teaching/32b-sum18/session-4.pdf</a></li>
<li>Khan Academy : <a href="https://ko.khanacademy.org/math/linear-algebra/matrix-transformations/determinant-depth/v/linear-algebra-determinant-and-area-of-a-parallelogram">행렬식과 평행사변형의 영역</a>, <a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives">Derivatives of multivariable functions</a></li>
<li><a href="http://www.mathstorehouse.com/archives/419">matrix
determinant lemma</a></li>
</ul>
</li>
<li>튜토리얼
<ul>
<li><a href="https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html#models-with-normalizing-flows">겁나 정리 잘한 블로그</a></li>
<li>강의안이 그림 캡쳐한 블로그 <a href="https://blog.evjang.com/2018/01/nf1.html">글 1</a>, <a href="https://blog.evjang.com/2018/01/nf2.html">글 2</a></li>
<li><a href="https://tiao.io/post/building-probability-distributions-with-tensorflow-probability-bijector-api/">tensorflow bijector API 사용법</a> 소개글</li>
</ul>
</li>
</ul>
<doc-anchor-target id="generative-adversarial-networks">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#generative-adversarial-networks">#</doc-anchor-trigger>
        <span>Generative Adversarial Networks</span>
    </h2>
</doc-anchor-target>
<doc-anchor-target id="energy-based-models">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#energy-based-models">#</doc-anchor-trigger>
        <span>Energy-based models</span>
    </h2>
</doc-anchor-target>

                                
                                <!-- Required only on API pages -->
                                <doc-toolbar-member-filter-no-results />
                            </div>
                            <footer class="clear-both">
                            
                                <nav class="flex mt-14">
                                    <div class="w-1/2">
                                        <a class="px-5 py-4 h-full flex items-center break-normal font-medium text-blue-500 dark:text-blue-400 border border-gray-300 hover:border-gray-400 dark:border-dark-650 dark:hover:border-dark-450 rounded-l-lg transition-colors duration-150 relative hover:z-5" href="../../../dl-paper/doc/etc/">
                                            <svg xmlns="http://www.w3.org/2000/svg" class="mr-3" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M19 11H7.41l5.29-5.29a.996.996 0 10-1.41-1.41l-7 7a1 1 0 000 1.42l7 7a1.024 1.024 0 001.42-.01.996.996 0 000-1.41L7.41 13H19c.55 0 1-.45 1-1s-.45-1-1-1z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
                                            <span>
                                                <span class="block text-xs font-normal text-gray-400 dark:text-dark-400">Previous</span>
                                                <span class="block mt-1">Etc.</span>
                                            </span>
                                        </a>
                                    </div>
                            
                                    <div class="w-1/2">
                                        <a class="px-5 py-4 -mx-px h-full flex items-center justify-end break-normal font-medium text-blue-500 dark:text-blue-400 border border-gray-300 hover:border-gray-400 dark:border-dark-650 dark:hover:border-dark-450 rounded-r-lg transition-colors duration-150 relative hover:z-5" href="../../../dl-paper/doc/img2img_translation/">
                                            <span>
                                                <span class="block text-xs font-normal text-right text-gray-400 dark:text-dark-400">Next</span>
                                                <span class="block mt-1">Image-​to-​image translation</span>
                                            </span>
                                            <svg xmlns="http://www.w3.org/2000/svg" class="ml-3" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M19.92 12.38a1 1 0 00-.22-1.09l-7-7a.996.996 0 10-1.41 1.41l5.3 5.3H5c-.55 0-1 .45-1 1s.45 1 1 1h11.59l-5.29 5.29a.996.996 0 000 1.41c.19.2.44.3.7.3s.51-.1.71-.29l7-7c.09-.09.16-.21.21-.33z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
                                        </a>
                                    </div>
                                </nav>
                            </footer>
                        </main>
                
                        <div class="border-t dark:border-dark-650 pt-6 mb-8">
                            <footer class="flex flex-wrap items-center justify-between">
                                <div>
                                    <ul class="flex flex-wrap items-center text-sm">
                                    </ul>
                                </div>
                                <div class="docs-copyright py-2 text-gray-500 dark:text-dark-350 text-sm leading-relaxed"><p>© Copyright 2020 - 2022 KyuHwan Shim, All rights reserved.</p></div>
                            </footer>
                        </div>
                    </div>
                
                    <!-- Rendered if sidebar right is enabled -->
                    <!-- Sidebar right skeleton-->
                    <div v-cloak class="fixed top-0 bottom-0 right-0 translate-x-full bg-white border-gray-200 lg:sticky lg:border-l lg:shrink-0 lg:pt-6 lg:transform-none sm:w-1/2 lg:w-64 lg:z-0 md:w-104 sidebar-right skeleton dark:bg-dark-850 dark:border-dark-650">
                        <div class="pl-5">
                            <div class="w-32 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                            <div class="w-48 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                            <div class="w-40 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                        </div>
                    </div>
                
                    <!-- User should be able to hide sidebar right -->
                    <doc-sidebar-right v-cloak></doc-sidebar-right>
                </div>

            </div>
        </div>
    
        <doc-search-mobile></doc-search-mobile>
        <doc-back-to-top></doc-back-to-top>
    </div>


    <div id="docs-overlay-target"></div>

    <script>window.__DOCS__ = { "title": "Generative Models", level: 3, icon: "file", hasPrism: false, hasMermaid: false, hasMath: false }</script>
</body>
</html>
