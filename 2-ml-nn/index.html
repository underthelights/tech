<!DOCTYPE html>
<html lang="en" class="h-full">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="turbo-root" content="/tech">
    <meta name="turbo-cache-control" content="no-cache">

    <!-- Primary Meta Tags -->
    <title>#ML 2.Neural Network</title>
    <meta name="title" content="#ML 2.Neural Network">
    <meta name="description" content="Part Ⅱ. Neural Networks" />

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="/tech/2-ml-nn/">
    <meta property="og:title" content="#ML 2.Neural Network">
    <meta property="og:description" content="Part Ⅱ. Neural Networks">
    <meta property="og:image" content="https://user-images.githubusercontent.com/46957634/122730805-9a9fa780-d2b5-11eb-8447-559a24fa7086.png">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="/tech/2-ml-nn/">
    <meta property="twitter:title" content="#ML 2.Neural Network">
    <meta property="twitter:description" content="Part Ⅱ. Neural Networks">
    <meta property="twitter:image" content="https://user-images.githubusercontent.com/46957634/122730805-9a9fa780-d2b5-11eb-8447-559a24fa7086.png">
    
    <script>(function () { var el = document.documentElement, m = localStorage.getItem("/tech/doc_theme"), wm = window.matchMedia; if (m === "dark" || (!m && wm && wm("(prefers-color-scheme: dark)").matches)) { el.classList.add("dark") } else { el.classList.remove("dark") } })();</script>

    <link href="/tech/favicon.ico" rel="icon" />
    <link href="/tech/resources/css/retype.css?v=1.9.683899159482" rel="stylesheet" data-turbo-track="reload" />

    <script type="text/javascript" src="/tech/resources/js/config.js?v=1.9.683899159482" defer data-turbo-track="reload"></script>
    <script type="text/javascript" src="/tech/resources/js/retype.js?v=1.9" defer data-turbo-track="reload"></script>
</head>
    <body>
        <div id="docs-app" class="relative text-base antialiased text-gray-700 bg-white font-body dark:bg-dark-850 dark:text-dark-300">
    <div class="absolute bottom-0 left-0 bg-gray-100 dark:bg-dark-800" style="top: 5rem; right: 50%"></div>

    <header id="docs-site-header" class="sticky top-0 z-30 flex w-full h-16 bg-white border-b border-gray-200 md:h-20 dark:bg-dark-850 dark:border-dark-650">
    <div class="container relative flex items-center justify-between flex-grow pr-6 md:justify-start">
        <!-- Mobile menu button skeleton -->
        <button v-cloak class="skeleton docs-mobile-menu-button flex items-center justify-center flex-shrink-0 overflow-hidden dark:text-white focus:outline-none rounded-full w-10 h-10 ml-3.5 md:hidden"><svg xmlns="http://www.w3.org/2000/svg" class="mb-px flex-shrink-0" width="24" height="24" viewBox="0 0 24 24" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor"><path d="M2 4h20v2H2zM2 11h20v2H2zM2 18h20v2H2z"></path></g></svg></button>
        <div v-cloak id="docs-sidebar-toggle"></div>

        <!-- Logo -->
        <div class="flex items-center justify-between h-full py-2 md:w-75">
            <div class="flex items-center px-2 md:px-6">
                <a id="docs-site-logo" href="/tech/" class="flex items-center leading-snug text-2xl">
                    <span class="w-10 mr-2 flex-grow-0 flex-shrink-0 overflow-hidden">
                        <img class="max-h-10 dark:hidden md:inline-block" src="/tech/static/retype-logo.svg">
                        <img class="max-h-10 hidden dark:inline-block" src="/tech/static/retype-logo-dark.svg">
                    </span>
                    <span class="dark:text-white font-semibold line-clamp-1 md:line-clamp-2">S.KYUHWN</span>
                </a>
            </div>

            <span class="hidden h-8 border-r md:inline-block dark:border-dark-650"></span>
        </div>

        <div class="flex justify-between md:flex-grow">
            <!-- Top Nav -->
            <nav class="hidden md:flex">
    <ul class="flex flex-col mb-4 md:pl-16 md:mb-0 md:flex-row md:items-center">
        <li class="md:mr-6">
            <a class="block py-2 text-sm text-gray-400 whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200 md:mb-0" href="/tech/">Home</a>
        </li>
        <li class="md:mr-6">
            <a class="block py-2 text-sm text-gray-400 whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200 md:mb-0" href="https://github.com/underthelights/">GitHub</a>
        </li>
        <li class="md:mr-6">
            <a class="block py-2 text-sm text-gray-400 whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200 md:mb-0" href="https://drive.google.com/file/d/1lcEg4fXJvTQlCKW5mWai_ej8NZGg21yT/view">Resume</a>
        </li>
        <li class="md:mr-6">
            <a class="block py-2 text-sm text-gray-400 whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200 md:mb-0" href="https://www.linkedin.com/in/kyuhwan-shim/">Linkedin</a>
        </li>
        <li class="md:mr-6">
            <a class="block py-2 text-sm text-gray-400 whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200 md:mb-0" href="https://underthelights.github.io">GitBlog</a>
        </li>
    </ul>
</nav>

            <!-- Header Right Skeleton -->
            <div v-cloak class="flex justify-end flex-grow skeleton">

                <!-- Search input mock -->
                <div class="relative hidden w-40 lg:block lg:max-w-sm lg:ml-auto">
                    <div class="absolute flex items-center justify-center h-full pl-3 dark:text-dark-300">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon-base" width="16" height="16" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation"  style="margin-bottom: 1px;"><g fill="currentColor" ><path d="M21.71 20.29l-3.68-3.68A8.963 8.963 0 0020 11c0-4.96-4.04-9-9-9s-9 4.04-9 9 4.04 9 9 9c2.12 0 4.07-.74 5.61-1.97l3.68 3.68c.2.19.45.29.71.29s.51-.1.71-.29c.39-.39.39-1.03 0-1.42zM4 11c0-3.86 3.14-7 7-7s7 3.14 7 7c0 1.92-.78 3.66-2.04 4.93-.01.01-.02.01-.02.01-.01.01-.01.01-.01.02A6.98 6.98 0 0111 18c-3.86 0-7-3.14-7-7z" ></path></g></svg>
                    </div>

                    <input class="w-full h-10 transition-colors duration-200 ease-in bg-gray-200 border border-transparent rounded md:text-sm hover:bg-white hover:border-gray-300 focus:outline-none focus:bg-white focus:border-gray-500 dark:bg-dark-600 dark:border-dark-600 placeholder-gray-400 dark:placeholder-dark-400"
                    style="padding: 0.625rem 0.75rem 0.625rem 2rem" type="text" placeholder="Search docs" />
                </div>

                <!-- Mobile search button mock -->
                <div class="flex items-center justify-center w-10 h-10 lg:hidden">
                    <svg xmlns="http://www.w3.org/2000/svg" class="flex-shrink-0 icon-base" width="20" height="20" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation"  style="margin-bottom: 0px;"><g fill="currentColor" ><path d="M21.71 20.29l-3.68-3.68A8.963 8.963 0 0020 11c0-4.96-4.04-9-9-9s-9 4.04-9 9 4.04 9 9 9c2.12 0 4.07-.74 5.61-1.97l3.68 3.68c.2.19.45.29.71.29s.51-.1.71-.29c.39-.39.39-1.03 0-1.42zM4 11c0-3.86 3.14-7 7-7s7 3.14 7 7c0 1.92-.78 3.66-2.04 4.93-.01.01-.02.01-.02.01-.01.01-.01.01-.01.02A6.98 6.98 0 0111 18c-3.86 0-7-3.14-7-7z" ></path></g></svg>
                </div>

                <!-- Dark mode switch placehokder -->
                <div class="w-10 h-10 lg:ml-2"></div>

                <!-- History button mock -->
                <div class="flex items-center justify-center w-10 h-10" style="margin-right: -0.625rem;">
                    <svg xmlns="http://www.w3.org/2000/svg" class="flex-shrink-0 icon-base" width="22" height="22" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation"  style="margin-bottom: 0px;"><g fill="currentColor" ><g ><path d="M12.01 6.01c-.55 0-1 .45-1 1V12a1 1 0 00.4.8l3 2.22a.985.985 0 001.39-.2.996.996 0 00-.21-1.4l-2.6-1.92V7.01c.02-.55-.43-1-.98-1z"></path><path d="M12.01 1.91c-5.33 0-9.69 4.16-10.05 9.4l-.29-.26a.997.997 0 10-1.34 1.48l1.97 1.79c.19.17.43.26.67.26s.48-.09.67-.26l1.97-1.79a.997.997 0 10-1.34-1.48l-.31.28c.34-4.14 3.82-7.41 8.05-7.41 4.46 0 8.08 3.63 8.08 8.09s-3.63 8.08-8.08 8.08c-2.18 0-4.22-.85-5.75-2.4a.996.996 0 10-1.42 1.4 10.02 10.02 0 007.17 2.99c5.56 0 10.08-4.52 10.08-10.08.01-5.56-4.52-10.09-10.08-10.09z"></path></g></g></svg>
                </div>
            </div>

            <div v-cloak class="flex items-center justify-end flex-grow">
                <div id="docs-mobile-search-button"></div>
                <doc-search-desktop></doc-search-desktop>

                <doc-theme-switch class="lg:ml-2"></doc-theme-switch>
                <doc-history></doc-history>
            </div>
        </div>
    </div>
</header>


    <div class="container relative flex bg-white">
        <!-- Sidebar Skeleton -->
<div v-cloak class="fixed flex flex-col flex-shrink-0 duration-300 ease-in-out bg-gray-100 border-gray-200 sidebar top-20 w-75 border-r h-screen md:sticky transition-transform skeleton dark:bg-dark-800 dark:border-dark-650">

    <!-- Render this div, if config.showSidebarFilter is `true` -->
    <div class="flex items-center h-16 px-6">
        <input class="w-full h-8 px-3 py-2 transition-colors duration-200 ease-linear bg-white border border-gray-200 rounded shadow-none text-sm focus:outline-none focus:border-gray-600 dark:bg-dark-600 dark:border-dark-600" type="text" placeholder="Filter" />
    </div>

    <div class="pl-6 mb-4 mt-1">
        <div class="w-32 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
        <div class="w-48 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
        <div class="w-40 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
        <div class="w-32 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
        <div class="w-48 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
        <div class="w-40 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
    </div>

    <div class="flex-shrink-0 mt-auto bg-transparent dark:border-dark-650">
        <a
    class="flex items-center justify-center flex-nowrap h-16 text-gray-400 dark:text-dark-400 hover:text-gray-700 dark:hover:text-dark-300 transition-colors duration-150 ease-in"
    target="_blank"
    href="https://retype.com/"
    rel="noopener"
>
    <span class="text-xs whitespace-nowrap">Powered by</span>
    <svg xmlns="http://www.w3.org/2000/svg" class="ml-2" fill="currentColor" width="96" height="20" overflow="visible"><path d="M0 0v20h13.59V0H0zm11.15 17.54H2.44V2.46h8.71v15.08zM15.8 20h2.44V4.67L15.8 2.22zM20.45 6.89V20h2.44V9.34z"/><g><path d="M40.16 8.44c0 1.49-.59 2.45-1.75 2.88l2.34 3.32h-2.53l-2.04-2.96h-1.43v2.96h-2.06V5.36h3.5c1.43 0 2.46.24 3.07.73s.9 1.27.9 2.35zm-2.48 1.1c.26-.23.38-.59.38-1.09 0-.5-.13-.84-.4-1.03s-.73-.28-1.39-.28h-1.54v2.75h1.5c.72 0 1.2-.12 1.45-.35zM51.56 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92h4.74v1.83h-6.79V5.36h6.64zM60.09 7.15v7.48h-2.06V7.15h-2.61V5.36h7.28v1.79h-2.61zM70.81 14.64h-2.06v-3.66l-3.19-5.61h2.23l1.99 3.45 1.99-3.45H74l-3.19 5.61v3.66zM83.99 6.19c.65.55.97 1.4.97 2.55s-.33 1.98-1 2.51-1.68.8-3.04.8h-1.23v2.59h-2.06V5.36h3.26c1.42 0 2.45.28 3.1.83zm-1.51 3.65c.25-.28.37-.69.37-1.22s-.16-.92-.48-1.14c-.32-.23-.82-.34-1.5-.34H79.7v3.12h1.38c.68 0 1.15-.14 1.4-.42zM95.85 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92H96v1.83h-6.79V5.36h6.64z"/></g></svg>
</a>

    </div>
</div>

<!-- Sidebar component -->
<doc-sidebar v-cloak>
    <template #sidebar-footer>
        <div
            class="flex-shrink-0 mt-auto border-t md:bg-transparent md:border-none dark:border-dark-650"
        >

            <div class="py-3 px-6 md:hidden border-b dark:border-dark-650">
                <nav>
                    <ul class="flex flex-wrap justify-center items-center">
                        <li class="mr-6">
                            <a class="block py-1 text-sm whitespace-nowrap transition-colors duration-200 ease-linear text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="/README.md">Home</a>
                        </li>
                        <li class="mr-6">
                            <a class="block py-1 text-sm whitespace-nowrap transition-colors duration-200 ease-linear text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://github.com/underthelights/">GitHub</a>
                        </li>
                        <li class="mr-6">
                            <a class="block py-1 text-sm whitespace-nowrap transition-colors duration-200 ease-linear text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://drive.google.com/file/d/1lcEg4fXJvTQlCKW5mWai_ej8NZGg21yT/view">Resume</a>
                        </li>
                        <li class="mr-6">
                            <a class="block py-1 text-sm whitespace-nowrap transition-colors duration-200 ease-linear text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://www.linkedin.com/in/kyuhwan-shim/">Linkedin</a>
                        </li>
                        <li class="mr-6">
                            <a class="block py-1 text-sm whitespace-nowrap transition-colors duration-200 ease-linear text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://underthelights.github.io">GitBlog</a>
                        </li>
                    </ul>
                </nav>
            </div>

            <a
    class="flex items-center justify-center flex-nowrap h-16 text-gray-400 dark:text-dark-400 hover:text-gray-700 dark:hover:text-dark-300 transition-colors duration-150 ease-in"
    target="_blank"
    href="https://retype.com/"
    rel="noopener"
>
    <span class="text-xs whitespace-nowrap">Powered by</span>
    <svg xmlns="http://www.w3.org/2000/svg" class="ml-2" fill="currentColor" width="96" height="20" overflow="visible"><path d="M0 0v20h13.59V0H0zm11.15 17.54H2.44V2.46h8.71v15.08zM15.8 20h2.44V4.67L15.8 2.22zM20.45 6.89V20h2.44V9.34z"/><g><path d="M40.16 8.44c0 1.49-.59 2.45-1.75 2.88l2.34 3.32h-2.53l-2.04-2.96h-1.43v2.96h-2.06V5.36h3.5c1.43 0 2.46.24 3.07.73s.9 1.27.9 2.35zm-2.48 1.1c.26-.23.38-.59.38-1.09 0-.5-.13-.84-.4-1.03s-.73-.28-1.39-.28h-1.54v2.75h1.5c.72 0 1.2-.12 1.45-.35zM51.56 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92h4.74v1.83h-6.79V5.36h6.64zM60.09 7.15v7.48h-2.06V7.15h-2.61V5.36h7.28v1.79h-2.61zM70.81 14.64h-2.06v-3.66l-3.19-5.61h2.23l1.99 3.45 1.99-3.45H74l-3.19 5.61v3.66zM83.99 6.19c.65.55.97 1.4.97 2.55s-.33 1.98-1 2.51-1.68.8-3.04.8h-1.23v2.59h-2.06V5.36h3.26c1.42 0 2.45.28 3.1.83zm-1.51 3.65c.25-.28.37-.69.37-1.22s-.16-.92-.48-1.14c-.32-.23-.82-.34-1.5-.34H79.7v3.12h1.38c.68 0 1.15-.14 1.4-.42zM95.85 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92H96v1.83h-6.79V5.36h6.64z"/></g></svg>
</a>

        </div>
    </template>
</doc-sidebar>


        <div class="flex-grow min-w-0 dark:bg-dark-850">
            <!-- Render "toolbar" template here on api pages --><!-- Render page content -->
            <div class="flex">
    <div class="flex-grow min-w-0 px-6 md:px-16">
        <main class="relative pt-6 pb-16">
            <div class="docs-markdown" id="docs-content">
                <!-- Rendered if sidebar right is enabled -->
                <div id="docs-sidebar-right-toggle"></div>
               
                <!-- Page content  -->
<doc-anchor-target id="ml-2neural-network" class="break-words">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#ml-2neural-network">#</doc-anchor-trigger>
        <span>#ML 2.Neural Network</span>
    </h1>
</doc-anchor-target>
<div class="-mt-3 mb-12 flex flex-wrap text-sm text-gray-400 dark:text-dark-350">
    <div class="flex items-center flex-shrink-0">
        <div>In&nbsp;</div>
        <a href="/tech/categories/ml/">ml</a>
    </div>
</div>

<p>Part Ⅱ. Neural Networks</p>
<p>ⓒ <a href="https://blog.naver.com/laonple">라온피플</a>, Stanford cs231n, Cornell Lecture Notes</p>
<doc-anchor-target id="2-neural-network">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#2-neural-network">#</doc-anchor-trigger>
        <span>2. Neural Network</span>
    </h1>
</doc-anchor-target>
<doc-anchor-target id="1-overview">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#1-overview">#</doc-anchor-trigger>
        <span>1. Overview</span>
    </h2>
</doc-anchor-target>
<doc-anchor-target id="11-why-neural-network">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#11-why-neural-network">#</doc-anchor-trigger>
        <span>1.1. Why Neural Network</span>
    </h3>
</doc-anchor-target>
<ul>
<li><p>인간의 두뇌와 유사한 제약조건 속 문제를 해결하는 것은 인공지능 문제에 대한 해결책이 될 수 있다.</p>
<ul>
<li><p>개별 neuron이 매우 느리게 작동할 때</p>
<p>하지만 두뇌는 복잡한 작업을 빠르게 수행합니다. <span class="math">\(\rightarrow\)</span> 대규모 병렬 알고리즘</p>
</li>
<li><p>Neuron은 고장이 발생하기 쉬운 장치</p>
<p>그러나 두뇌는 믿을 만 합니다 <span class="math">\(\rightarrow\)</span> Distributive Representation을 믿을 수 있다</p>
</li>
<li><p>Neuron은 대략적으로 매칭을 촉진시킨다</p>
<p>약간 불안정 <span class="math">\(\rightarrow\)</span> 학습 가능</p>
</li>
</ul>
</li>
</ul>
<doc-anchor-target id="12-biological-neural-network">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#12-biological-neural-network">#</doc-anchor-trigger>
        <span>1.2. Biological Neural Network</span>
    </h3>
</doc-anchor-target>
<ul>
<li><p>from. cornell cs lecture notes</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46957634/122730805-9a9fa780-d2b5-11eb-8447-559a24fa7086.png" alt="image" /></p>
</li>
<li><p>Neural signals propagated via complicated electrochemical reaction.</p>
</li>
<li><p>Cell body</p>
<ul>
<li><p>one axon</p>
<ul>
<li>delivers output to other connect neurons</li>
<li>single long fiber ; 100 or more times the diameter of cell body.</li>
<li>connects via synapses to dendrites of other cells.</li>
</ul>
</li>
<li><p>many dendrites</p>
<ul>
<li>외부로부터의 신경 자극을 받아들이는 역할</li>
</ul>
</li>
<li><p>신경세포체 (Soma)</p>
<ul>
<li>신경 세포의 핵을 담당하는 부분</li>
<li>여러 뉴런으로부터 전달되는 외부 자극에 대한 판정을 하여 다른 뉴런으로 신호를 전달할 것인지를 최종 결정을 한다.</li>
</ul>
</li>
<li><p>시냅스 (Synapse)</p>
<ul>
<li>어떤 뉴런의 축삭돌기 말단과 다음 뉴런의 수상돌기의 연결 부위를 말한다.</li>
<li>이 시냅스는 얇은 막의 형태이며, 다른 뉴런의 축삭돌기로부터 받는 신호를 어느 정도의 세기(strength, weight)로 전달할 것인지를 결정한다.</li>
<li>excitatory, inhibitory</li>
</ul>
</li>
</ul>
</li>
<li><p>Each neuron is a “threshold unit”. 적당한 임계점에 도달하기 전 까지 Neuron은 아무 것도 하지 않는다.</p>
</li>
<li><p>최대 강도 출력(Full strength output)생산 : 특정 syanpse에서의 자극은 neuron들이 발화하거나, 발화하지 못하게 한다.</p>
</li>
</ul>
<doc-anchor-target id="13--structure-of-ann">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#13--structure-of-ann">#</doc-anchor-trigger>
        <span>1.3.  Structure of ANN</span>
    </h3>
</doc-anchor-target>
<p><img src="https://user-images.githubusercontent.com/46957634/122732410-354cb600-d2b7-11eb-83df-69722f238bde.png" alt="image" /></p>
<ul>
<li><p>ANN imitates the structure of biological neuron</p>
</li>
<li><p>학습을 하게 되면, 한 신경 세포에서 다른 신경 세포로 연결되는 시냅스 부분에서 신호의 세기가 결정이 되고 굳어지게 된다. 즉, 특정 자극에 학습의 기대치대로 결과를 낼 수 있도록 각각의 시냅스의 세기가 결정이 된다.</p>
</li>
<li><p>수상돌기에 해당되는 입력에 <span class="math">\((x_1, x_2, …, x_n)\)</span> 시냅스에 해당하는 입력의 가중치 $(w_1, w_2, …, w_n)$를 곱하고 이것들의 총합이 신경세포체(Soma) 부분으로 전달이 되면, 신경 세포체에서는 활성함수(activation function)에 따라 최종 출력 $Y$가 결정이 된다.</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46957634/122732751-8e1c4e80-d2b7-11eb-8b5d-8249595a9b94.png" alt="image" /></p>
</li>
<li><p>위 그림에서는 활성 함수가 특정 경계값(threshold)과 비교를 하여 같거나 크면 ‘<span class="math">\(+1\)</span>’을 출력하고 작으면 ‘<span class="math">\(-1\)</span>’을 출력한다.</p>
</li>
<li><p>인공신경망(ANN)은 보통 이런 뉴런들을 multi-layer로 구성을 하며, 뒤에서 설명하게 될 역전파(back-propagation) 알고리즘을 통해 신경망의 학습 결과가 기대치와 비슷한 결과를 낼 수 있도록 뉴런의 입력으로 들어오는 시냅스의 가중치를 계속 조절해가는 과정을 거친다.</p>
</li>
<li><p>이것을 훈련(training)이라고 하며, 훈련 데이터를 통한 반복 훈련을 통해 가중치(w1, w2, …, wn)의 최적값이 정해지게 된다.</p>
</li>
<li><p>현재 인공신경망은 문자 인식, 필기체 인식, 음성 인식 등의 분야에서 놀라운 성과를 보이고 있으며, 경제학적 모델을 해석하는데도, 석유의 연간 소비량을 예측하는데도, 영상이나 동영상의 자동 태깅 분야에서도 좋은 결과를 내고 있다.</p>
</li>
</ul>
<doc-anchor-target id="2-history">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#2-history">#</doc-anchor-trigger>
        <span>2. History</span>
    </h2>
</doc-anchor-target>
<doc-anchor-target id="21-neural-network신경망-역사">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#21-neural-network신경망-역사">#</doc-anchor-trigger>
        <span>2.1. Neural Network(신경망) 역사</span>
    </h3>
</doc-anchor-target>
<ul>
<li><p>Neural Network의 역사는 다른 과학의 분야와 마찬가지로 오랜 개발 기간이 소요되었으며, 그 동안 여러 번의 부침이 있었다. 신경망의 역사는 1940년대 초반으로 거슬러 올라갈 수 있으며, 컴퓨터 개발의 역사와 거의 비슷하다고 볼 수 있다. 그래서 초반의 인물들을 살펴보면, 많은 사람들이 컴퓨터의 개발자들과 겹치는 것을 볼 수 있다.</p>
</li>
<li><p>신경망의 역사를 구분하는 방식은 사람마다 견해가 다르지만, 여기서는 4개의 시기로 나누는 방식을 따른다.</p>
<ol>
<li><doc-anchor-trigger to="#211-%ED%83%9C%EB%8F%99%EA%B8%B0">태동기</doc-anchor-trigger></li>
<li><doc-anchor-trigger to="#212-%ED%99%A9%EA%B8%88%EA%B8%B0">황금기</doc-anchor-trigger></li>
<li><doc-anchor-trigger to="#213-%EA%B8%B4-%EC%B9%A8%EB%AC%B5%EA%B8%B0">긴 침묵기</doc-anchor-trigger></li>
<li><doc-anchor-trigger to="#214-">르네상스기</doc-anchor-trigger></li>
</ol>
</li>
</ul>
<doc-anchor-target id="211-태동기">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#211-태동기">#</doc-anchor-trigger>
        <span>2.1.1. 태동기</span>
    </h4>
</doc-anchor-target>
<ul>
<li><p>1943년. introduced ANN, connect neural circuity and logic by McCulloch &amp; Pitts</p>
<ul>
<li>McCulloch, W. S. and Pitts, W. H. (1943). <a href="https://www.cs.cmu.edu/%7E./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf">A logical calculus of the ideas immanent in nervous activity</a></li>
<li>2가지 관점
<ol>
<li>Modeling Brain</li>
<li>Representation of complex functions</li>
</ol>
</li>
<li>McCulloch와 Pitts는 산술연산과 논리연산을 수행할 수 있는 간단한 신경 망에 기초한 연산 모델(computational model) 제작</li>
</ul>
</li>
<li><p>1949년 : Hebbian Learning</p>
<ul>
<li><a href="https://link.springer.com/chapter/10.1007/978-3-642-70911-1_15">Donald Hebb: The Organization of Behavior</a></li>
<li>Learning Law based on the synapse of neuron</li>
</ul>
</li>
</ul>
<doc-anchor-target id="212-황금기">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#212-황금기">#</doc-anchor-trigger>
        <span>2.1.2. 황금기</span>
    </h4>
</doc-anchor-target>
<ul>
<li><p>1950년대와 60년대는 신경망의 황금기로 불린다.</p>
</li>
<li><p>1951년 Minsky는 Snark라는 이름을 갖는 neurocomputer를 개발했으며, 가중치(weight)을 자동으로 조절할 수가 있었다. 하지만 실제로 구현이 되지는 못했다.</p>
</li>
<li><p>Checkers (1952)</p>
<ul>
<li>from. <a href="https://ieeexplore.ieee.org/document/5389202?arnumber=5389202">Some studies in machine learning using the game of checkers</a></li>
<li>Samuel’s program learned weights and played at strong amateur level</li>
<li>first machine learning system that received public recognition</li>
</ul>
</li>
<li><p>Problem solving (1955)</p>
<ul>
<li><p>Newell &amp; Simon’s Logic Theorist</p>
<ul>
<li>prove theorems in Principia Mathematica using search + heuristics; later, <a href="https://en.wikipedia.org/wiki/General_Problem_Solver">General Problem Solver (GPS)</a></li>
<li>상식적 추론 과정을 논리로서 암호화하는 과정을 통해 일반성 추구</li>
</ul>
</li>
</ul>
</li>
<li><p>1956: Workshop at Dartmouth College; attendees: John McCarthy, Marvin Minsky, Claude Shannon, etc.</p>
<ul>
<li><p>Aim for general principles:</p>
<blockquote>
<p>Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it</p>
<p>학습의 모든 측면 또는 지능의 다른 특징은 기계가 그것을 시뮬레이션할 수 있도록 매우 정밀하게 설명될 수 있다.</p>
</blockquote>
</li>
<li><p>MIT에 있었지만 나중에 Stanford AI Lab을 설립한 John McCarthy는 Dartmouth U.에서 그 시대의 주요 사상가들과 함께 워크숍을 조직했고, 모든 것을 할 수 있는 시스템을 구축하기 위해서 대담한 제안을 내놓았다.</p>
</li>
</ul>
</li>
<li><p>1957 : Perceptron (by Frank Rosenblatt)</p>
<ul>
<li><img src="https://user-images.githubusercontent.com/46957634/122730528-4694c300-d2b5-11eb-8cfd-4b211c668975.png" alt="image" /></li>
<li><a href="https://link.springer.com/chapter/10.1007/978-3-642-70911-1_20">Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms&quot;, by Frank Rosenblatt</a></li>
<li>비교적 정확히 기술된, 계산에 의한 최초의 신경망 모델</li>
<li>Rosenblatt는 신경망에서 흔히 사용되는 “perceptron”이라는 용어 및 알고리즘을 개발했고, 후에 이에 기반한 최초의 성공적인 Neuro-Computer를 개발했으며, 이것을 패턴 인식 분야에 적용하였다.</li>
<li>이 perceptron이 많은 일을 할 수 있을 것으로 전망이 되었지만, 얼마 안돼 제한이 많이 있음이 밝혀졌다. Single layer perceptron는 선형적으로 분리가 가능한 패턴은 인식할 수 있지만, 복잡한 패턴은 2개 혹은 그 이상의 layer를 갖는 신경망(multi-layer) 신경망에서 가능하다는 것이 1969년 Minsky에 의해 증명이 된다.</li>
<li>오늘날 perceptron은 한 쪽 혹은 다른 쪽에 속하는 것을 결정할 수 있는 binary classifier에 대한 지도 학습 알고리즘의 개념으로 사용이 되고 있다.</li>
</ul>
</li>
<li><p>1969 : XOR problem (by Minsky, Papert)</p>
<ul>
<li><a href="https://mitpress.mit.edu/books/perceptrons">Perceptrons: an introduction to computational geometry</a></li>
<li>Perceptrons showed that linear models couldn&#x27;t solve XOR</li>
<li>largely credited with the demise of neural networks research, and the continued rise of logical AI.</li>
</ul>
</li>
</ul>
<doc-anchor-target id="213-긴-침묵기">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#213-긴-침묵기">#</doc-anchor-trigger>
        <span>2.1.3. 긴 침묵기</span>
    </h4>
</doc-anchor-target>
<ul>
<li>1970년대에 들어서면서 연구비에 대한 지원이 줄고, 학회도 거의 없어지면서 논문이 출간되는 횟수도 크게 줄어들게 된다.</li>
<li>하지만, 개별적으로 신경망에 대한 연구는 지속이 되었으며, 독자적으로 신경망에 대한 패러다임을 발전시켰다. 그리고 이 시기의 연구들은 80년대 후반 르네상스 시대의 밑거름이 된다. 1976년 Grossberg는 많은 논문을 발표했으며, 그의 연구는 후에 Carpenter에 의해 ART(Adaptive resonance theory)로 발전을 하게 된다.</li>
<li>1982년 Kohonen은 Kohonen map이라고도 알려진 SOM(self-organization feature map)을 발표했다. 또한 Hopfield는 Hopfield 망을 발표하였다. 후에 SOM과 ART를 통하여 신경망을 자율학습(unsupervised learning) 분야에 적용을 할 수 있게 된다.</li>
</ul>
<doc-anchor-target id="214-르네상스기">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#214-르네상스기">#</doc-anchor-trigger>
        <span>2.1.4. 르네상스기</span>
    </h4>
</doc-anchor-target>
<ul>
<li><p>1982년과 1984년에 저명한 물리학자 Hopfield는 널리 읽히는 신경망 관련 논문 2편을 발표하였으며, 전세계를 돌면서 강의를 통해 신경망에 대한 연구를 활성화 시키기를 촉구한다. 이는 많은 연구자들이 다시 신경망에 관심을 갖게 되는 계기가 되었다.</p>
</li>
<li><p>1986년, Rumelhart와 Hinton이 드디어 그 유명한 back-propagation 알고리즘을 발표하면서 신경망은 많은 문제들을 풀 수 있게 되었다.</p>
</li>
<li><p>1987년에는 최초로 신경망에 대한 국제 conference가 열리게 되고, 그 다음 해에는 신경망에 대한 국제 journal이 만들어지게 되었다.</p>
</li>
<li><p>1995년 LeCun과 Bengio는 CNN(Convolutional Neural Network)를 발표한다.CNN을 이용하여 local invariant feature를 쉽게 추출이 가능하고,기존 신경망이 갖는 문제점을 극복할 수 있게 되었으며, 문자 인식이나 음성 인식 분야에서 탁월한 성능을 얻을 수 있게 되었다.</p>
</li>
<li><p>하지만 신경망이 갖는 복잡도나 적절한 hyper-parameter 설정이 없이는 좋은 결과를 기대할 수도 없고, 좋은 hyper-parameter 설정을 경험적인 방법에 많이 의지해야 하는 어려움이 있었다. 이로 인해 기계학습 분야에서 SVM(Support Vector Machine)이나 이것보다 훨씬 간단한 linear classifier 같은 알고리즘에 신경망이 점차 뒤로 밀리는 분위기가 되었다.</p>
</li>
<li><p>하지만, 2000년대 후반부터 부각된 딥러닝(deep learning) 때문에 다시 한번 신경망 연구에 불이 붙게 된다.</p>
</li>
</ul>
<doc-anchor-target id="3-basic-theory">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#3-basic-theory">#</doc-anchor-trigger>
        <span>3. Basic Theory</span>
    </h2>
</doc-anchor-target>
<doc-anchor-target id="31-hebbian-rule">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#31-hebbian-rule">#</doc-anchor-trigger>
        <span>3.1. Hebbian Rule</span>
    </h3>
</doc-anchor-target>
<ul>
<li>앞서, [신경망의 역사](2. History)에서 살펴보았던 것처럼, 1949년 심리학자인 Donald Hebb은 뉴런의 시냅스에 기반한 학습의 법칙을 발표하였다. 그는 생물학적인 신경망에서 학습이 이루어지면 특정 입력으로 들어오는 신호 자극에 잘 반응할 수 있도록 시냅스들의 세기가 결정이 된다는 사실에 주목했다. 그의 이름을 딴 Hebbian rule에 따르면, 학습이란 시냅스 연결의 세기(strength)를 조정하는 것으로 정의했으며, 기본적인 학습 방법은 2개의 뉴런이 동시에 활성화 시키려면, 뉴런이 연결된 가중치(weight)를 높이면 된다.</li>
</ul>
<doc-anchor-target id="32-perceptron">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#32-perceptron">#</doc-anchor-trigger>
        <span>3.2. Perceptron</span>
    </h3>
</doc-anchor-target>
<doc-anchor-target id="321-perceptron의-개념">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#321-perceptron의-개념">#</doc-anchor-trigger>
        <span>3.2.1. Perceptron의 개념</span>
    </h4>
</doc-anchor-target>
<ul>
<li>1957년에 Rosenbalt는 “Perceptron”이라는 용어 및 개념을 발표했다. 발표 당시, 뉴런의 활성함수(activation function)로 “step function”을 사용했기 때문에 지금처럼 “sigmoid function”을 사용하는 뉴런에 비해 제약이 많았지만, 입력의 중요도에 따라 출력이 결정이 되는 수학적 모델로서는 의미가 있다. 여기서 입력의 중요도는 가중치에 따라 결정된다는 개념이 도입이 되었으며, 아래 그림처럼, 2 layer feed forward 구조에서는 여러 개의 입력을 받아 1개의 출력을 결정하는 신경망의 경우를 살펴보면, 출력은 가중치와 입력의 곱의 합이 특정 기준(threshold) 보다 작으면 0이 되고, 크면 1을 출력한다.</li>
</ul>
<doc-anchor-target id="322-perceptron의-한계">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#322-perceptron의-한계">#</doc-anchor-trigger>
        <span>3.2.2. Perceptron의 한계</span>
    </h4>
</doc-anchor-target>
<ul>
<li><p>Rosenbalt의 Perceptron은 그 개념상 생물학적인 신경망의 특성을 잘 반영한 것 같지만, 한계가 있다. Perceptron의 문제는 뉴런의 활성화 함수가 step function이기 때문에, 출력이 0과 1처럼 극단적인 결과만을 도출할 수 있다. 2 layer 만으로 구성이 되거나, 아주 단순한 결과를 도출해야 하는 경우는 문제가 없지만, 뉴런의 출력이 0과 1로 극단적인 상황만 있기 때문에 다중 layer 신경망의 경우는 좋은 결과를 얻기가 어렵다.</p>
</li>
<li><p>최적의 학습 결과를 갖는 신경망을 설계하려면, 나중에 자세히 다루겠지만, 역전파(back-propagation)와 gradient-descent 방법을 사용한다. 이 개념은 근본적으로 입력이나 특정 넷의 가중치를 약간 변경시키면, 출력에 작은 변화가 일어난다는 점에 근거하고 있다. Perceptron 기반의 뉴런은 weight나 bias의 작은 변화가 출력 쪽에 작은 변화를 만들어내면서 섬세하게 신경망을 학습을 시킨다는 오늘날의 학습 개념과는 부합이 잘 안된다.</p>
</li>
</ul>
<doc-anchor-target id="33-sigmoid-functions">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#33-sigmoid-functions">#</doc-anchor-trigger>
        <span>3.3. Sigmoid Functions</span>
    </h3>
</doc-anchor-target>
<ul>
<li><p>해결책은 활성함수로 Perceptron처럼 Step function을 사용하는 대신에, “Sigmoid 함수”를 사용하는 것이다.</p>
</li>
<li><p>활성화 함수로 “Sigmoid 함수”를 사용하면, 0에서 1까지 연속적으로 변하는 출력값을 갖기 때문에, 가중치나 바이어스를 조금 변화시켰을 때 출력이 조금씩 변화하도록 만들 수 있다.</p>
</li>
<li><p>Sigmoid 함수는 아래와 같은 식을 갖는다.</p>
</li>
<li><p>여기서 z는 각각의 입력(x1, x2, x3, …)과 가중치(w1, w2, w3, ..)를 곱한 값에 bias를 더한 값이며, 입력이 결정이 되면, 가중치나 바이어스를 약간 변화시켰을 때(즉 편미분을 하였을 때), 출력이 그에 상응하여 변화하는 것을 확인할 수 있다.</p>
</li>
</ul>
<doc-anchor-target id="34-gradient-descent">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#34-gradient-descent">#</doc-anchor-trigger>
        <span>3.4. Gradient-Descent</span>
    </h3>
</doc-anchor-target>
<ul>
<li><p>여기서 중요한 점은 가중치나 바이어스의 작은 변화량에 대해서 출력의 변화량이 linear 하다는 점이다. 이런 선형적인 특성으로 인해, 가중치나 바이어스를 조금씩 바꾸면서 출력이 원하는 방향으로 움직이도록 만들 수 있다.</p>
</li>
<li><p>이것을 유식한 용어로는 Gradient-Descent 방법이라고 하며, 최적값을 찾아갈 때 흔히 사용하는 방법이다.</p>
</li>
<li><p>좀 더 부연설명을 하면, gradient-descent 방법은 오목한 그릇에서 공을 굴리는 경우를 생각하면 이해하기가 쉽다. 어떤 지점에서 굴리기를 시작할지라도, 그릇의 밑바닥까지 내려가면 최적값에 도달했다고 볼 수가 있다. 이 때 내려가는 방향을 선택하려면, 공이 특정 위치에 있을 때, 그 미분값(gradient)가 음이 되는 방향을 선택하면 된다. 이 과정을 반복하다 보면 공은 결국은 바닥에 내려가게 되듯이, 어떤 특정 위치에서 시작을 하더라도 그 위치에서 편미분의 값이 음수가 되는 방향을 계속 선택하면 최적값에 도달하게 된다.</p>
</li>
</ul>
<doc-anchor-target id="35-supervised-learning">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#35-supervised-learning">#</doc-anchor-trigger>
        <span>3.5. Supervised Learning</span>
    </h3>
</doc-anchor-target>
<ul>
<li><p>이것은 어떻게 보면 학습의 원리와 유사하다. 미리 값을 알고 있는 훈련 데이타를 통해, 가중치와 바이어스를 조금씩 변화시켜 가면서 출력이 최적의 상태가 되도록 하는 방법이 바로 지도 학습(supervised learning)이다.</p>
</li>
<li><p>가중치와 바이어스의 최적값을 찾아가는 방법은 얼핏 보면 쉬워 보이지만, 1개의 뉴런에 여러 개의 입력이 연결이 되고, 또 그런 뉴런이 여러 개가 있다면 변화시켜야 할 가중치와 바이어스가 점점 많아지게 된다.</p>
</li>
<li><p>어떻게 최적값을 찾아갈 수 있을까?</p>
</li>
<li><p>막연하게 막고 품는 방식으로 여러 개의 값들을 무작위로 바꿔 간다면 과연 최적값에 도달할 수 있을까?</p>
</li>
<li><p>이것에 대한 해답은 역전파(back-propagation) 방법을 사용하면 된다. 그리고 이 방법은 다음 class에서 자세하게 다루기로 한다.</p>
</li>
</ul>
<doc-anchor-target id="4-backpropagation-역전파">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#4-backpropagation-역전파">#</doc-anchor-trigger>
        <span>4. Backpropagation (역전파)</span>
    </h2>
</doc-anchor-target>
<doc-anchor-target id="41-backpropagation의-알고리즘의-기원">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#41-backpropagation의-알고리즘의-기원">#</doc-anchor-trigger>
        <span>4.1. Backpropagation의 알고리즘의 기원</span>
    </h3>
</doc-anchor-target>
<ul>
<li>신경망에서 가장 중요한 개념 중 하나가 바로 역전파(backpropagation)이다. 이 “역전파”를 통해 “역방향으로 에러를 전파(backward propagation of error)” 시키면서 최적의 학습 결과를 찾아가는 것이 가능해졌다. 참고로, 역전파 알고리즘은 1970년대 개발이 되었지만 (그 이전에 개발이 되었다고 주장하는 사람들도 있지만, 대략적으로 70년대로 봄), 1986년 Rumelhart와 Hinton의 논문을 통해 최적의 신경망 변수들을 찾아내는데 적합함이 증명이 되었고, 비로소 빛을 보게 되었으며, 신경망에 대한 연구에 다시 불을 붙이는 촉매제가 되었다.</li>
</ul>
<doc-anchor-target id="42-backpropagation-이해를-위한-사전-지식">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#42-backpropagation-이해를-위한-사전-지식">#</doc-anchor-trigger>
        <span>4.2. Backpropagation 이해를 위한 사전 지식</span>
    </h3>
</doc-anchor-target>
<doc-anchor-target id="421-cost-function-loss-function">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#421-cost-function-loss-function">#</doc-anchor-trigger>
        <span>4.2.1. Cost Function (Loss Function)</span>
    </h4>
</doc-anchor-target>
<ul>
<li><p>역전파에 대한 본격적인 설명에 앞서, “cost function” 혹은 “loss function”의 개념을 먼저 파악을 해야 할 필요가 있다.</p>
</li>
<li><p>cost function은 아래와 같이 정의된다.</p>
</li>
<li><p>여기에서 n은 훈련에 사용하는 입력의 수를 나타내고, y(x)는 입력 x를 가했을 때의 기대(target) 출력을 나타내며, a는 입력 x를 신경망에 넣었을 때의 실제 출력이다.</p>
</li>
<li><p>이 식으로부터 짐작할 수 있겠지만, cost function이란 신경망에 훈련 데이터 x를 가하고 실제 출력과 기대 출력간의 차에 대한 MSE(Mean Square Error)를 구하는 것임을 알 수가 있다.</p>
</li>
<li><p>y(x)와 a의 차가 작아질수록 신경망이 학습이 잘된다고 볼 수 있으며, 훈련 데이터를 이용해 가중치(w)와 바이어스(b)를 변화시키는 과정을 반복적으로 수행하여 cost function이 최소값이 되도록 하는 것이 신경망 학습의 목표이다.</p>
</li>
</ul>
<doc-anchor-target id="422-gradient-descent-방법에-기반한-학습">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#422-gradient-descent-방법에-기반한-학습">#</doc-anchor-trigger>
        <span>4.2.2. Gradient-descent 방법에 기반한 학습</span>
    </h4>
</doc-anchor-target>
<ul>
<li><p>위 식처럼, cost function이 정의가 된다면, 과연 어떻게 w와 b값들을 변화시켜야 최적의 결과를 얻을 수 있을까?</p>
</li>
<li><p>신경망의 뉴런의 개수가 많아지고, 입력의 수가 많아질수록 더욱 더 해를 구하기가 어려워진다. 이 때 필요한 개념이 gradient-descent 방법이다.</p>
</li>
<li><p>Gradient-descent에 대한 설명 중 위키에 나온 설명이 재미있게 잘 된 것 같다. 위키피디아의 설명에 따르면, &#x27; 안개가 끼어있는 산정상에서 한치 앞이 보이지 않는다면, 어떻게 내려가야 할까? 짙은 안개로 인해 앞이 잘 보이지 않는다면, 자기의 현재 위치에서의 정보 (local information)만을 활용해야 하며, 그 정보로는 가장 경사가 큰 쪽으로 내려가는 길을 택하면 된다. &#x27;어느 정도 위치에 바닥이 있는지는 정확하게 알 수은 없지만, 항상 현재 위치에서 기울기가 가장 큰 방향으로 내려간다면 결국 바닥에 도달할 수 있게 된다. 물론 local minimum에 빠질 수도 있겠지만, 단지 국소 정보만을 활용할 때는 가장 많이 사용하는 것이 gradient-descent이고, 최대값을 찾는 경우에는 gradient-ascent를 사용하면 된다. 마찬가지로, 훈련 데이터를 입력으로 가하고, cost function이 최소가 되도록 w와 b값들을 반복적으로 변화시켜가다 보면 결국, 최소값에 이를 수 있게 된다.</p>
</li>
<li><p>위 그림은 gradient-descent를 설명할 때 흔히 사용되는 그림으로, 어느 위치에서 시작을 할지라도 gradient가 음의 최대값 쪽으로 움직이게 되면, 결국 최소값이 도달할 수 있다.</p>
</li>
<li><p>단, 단점은 경사가 큰 경우는 빠른 속도로 수렴을 하지만, 거의 바닥에 오게 되면, 기울기가 작아지기 때문에 수렴 속도가 현저히 느려진다는 점이다.</p>
</li>
</ul>
<doc-anchor-target id="43-역전파backpropagation-기본-개념">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#43-역전파backpropagation-기본-개념">#</doc-anchor-trigger>
        <span>4.3. 역전파(backpropagation) 기본 개념</span>
    </h3>
</doc-anchor-target>
<ul>
<li><p>가중치(w)와 바이어스(b)값들을 조절하여 cost function이 최소가 되도록 하는 gradient-descent 개념을 이해 했다면, 어떻게 이 값들을 조절해야 할까?라는 의문이 생긴다.</p>
</li>
<li><p>Gradient Descent using Partial Derivatives</p>
<ul>
<li><span class="math">\(\frac {\partial E}{\partial W_j} = E_{rr} \cdot \frac{\partial E_{rr}}{\partial W_j}= E_{rr} \cdot \frac{\partial}{\partial W_j} (y - g(\sum_{j=0}^n {W_j x_j})) = -E_{rr} \cdot g'(in) \cdot x_j\)</span></li>
</ul>
</li>
<li><p>Update weights</p>
<ul>
<li><span class="math">\(W_j \leftarrow W_j + \alpha E_{rr} \cdot g'(in) \cdot x_j\)</span></li>
</ul>
</li>
<li><p>신경망의 크기가 커지고, 입력이나 출력의 개수가 많아지면, 변수들이 너무 많기 때문에 이 작업은 매우 곤혹스러운 작업이 된다. 앞서 [Part II. Neural Networks] 3. Basic Theory 에서 “가중치나 바이어스 값을 아주 작게 변화를 시키면,</p>
<p><span class="math">\(\rightarrow\)</span> 편미분을 시키면, 출력 쪽에서 생기는 변화 역시 매우 작은 변화가 생기며, 작은 구간만을 보았을 때는 선형적인 관계가 있다”라고 했음을 상기하자.</p>
<p>이 말을 곰곰이 생각을 해보면, 작은 변화의 관점에서는 선형적인 관계이기 때문에, 출력에서 생긴 오차를 반대로 입력 쪽으로 전파시키면서 w와 b등을 갱신하면 된다는 뜻이 된다.</p>
</li>
<li><p>w와 b 값들을 무작위로 변화시키는 것이 아니라, cost function이 결국 w와 b의 함수로 이루어졌기 때문에, 출력 부분부터 시작해서 입력 쪽으로, 즉 역 방향으로, 순차적으로 cost function에 대한 편미분을 구하고, 얻은 편미분 값을 이용해 w와 b의 값을 갱신시킨다.</p>
</li>
<li><p>모든 훈련 데이터에 대해서 이 작업을 반복적으로 수행을 하다 보면, 훈련 데이터에 최적화된 w와 b 값들을 얻을 수 있다. 역전파(backpropagation)란 용어는 출력부터 반대 방향으로 순차적으로 편미분을 수행해가면서 w와 b값들을 갱신시켜간다는 뜻에서 만들어진 것이다.</p>
</li>
</ul>
<doc-anchor-target id="44-뉴런의-재구성">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#44-뉴런의-재구성">#</doc-anchor-trigger>
        <span>4.4. 뉴런의 재구성</span>
    </h3>
</doc-anchor-target>
<ul>
<li>역전파를 쉽게 이해하기 위해서 보통 뉴런의 구조를 아래 그림과 같이 재 구성을 할 수가 있다.</li>
<li>즉, 각각의 넷으로부터 들어오는 입력을 합하는 부분과 활성화 함수 부분을 나누어 생각하면, 역전파를 위해 편미분을 할 때 좀 더 쉽게 이해를 할 수가 있다. 아래 그림에서 f(e)는 sigmoid 함수에 해당이 되며, e는 각 넷으로부터 입력과 가중치의 곱의 총합을 나타낸다.</li>
</ul>
<doc-anchor-target id="441-역전파의-기본-스텝--역전파는-보통-2단계를-거친다">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#441-역전파의-기본-스텝--역전파는-보통-2단계를-거친다">#</doc-anchor-trigger>
        <span>4.4.1. 역전파의 기본 스텝 : 역전파는 보통 2단계를 거친다.</span>
    </h4>
</doc-anchor-target>
<ul>
<li><p>먼저 feed forward 과정을 거치면서 입력이 최종 출력까지 전달되고 최종 출력단에서 에러와 cost function을 구한다.</p>
</li>
<li><p>다음은 backpropagation 단계이며, 최종단에서 구한 기대 출력과 실제 출력간의 차(에러)를 반대 방향으로 전파시키면서 각각 넷이나 뉴런의 가중치(w)와 바이어스(b) 값을 갱신한다. 이 과정을 좀 더 이해를 쉽게 하기 위해, 뉴런을 재구성하면 다음과 같다.</p>
</li>
<li><p>위 그림에서 s는 활성 함수, 즉 sigmoid 함수를 뜻하며, s’는 s의 미분 함수이다.</p>
</li>
<li><p>Feed forward 과정에서는 왼쪽에서 오른쪽으로 흐름이 만들어지며, 위 그림의 원에서 오른쪽 반원에 있는 부분에 해당된다.</p>
</li>
<li><p>즉, 모든 넷에서 들어오는 입력의 합을 sigmoid 함수를 거쳐서 출력을 함을 의미한다. Backpropagation을 하는 경우는 오른쪽에서 왼쪽으로 계산을 하며, 연산을 수행할 때는 왼쪽 반원에 있는 부분을 선택한다. 즉, 에러를 역전파할 때는 sigmoid 함수의 미분 함수 s’를 사용하고 각 net로 모두 동일하게 전파가 된다.</p>
</li>
</ul>
<doc-anchor-target id="442-역전파의-과정-종합">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#442-역전파의-과정-종합">#</doc-anchor-trigger>
        <span>4.4.2. 역전파의 과정 종합</span>
    </h4>
</doc-anchor-target>
<ul>
<li><p>앞서 설명한 것처럼, 역전파는 feed forward와 backpropagation의 2단계로 구성이 된다. Feed forward 단계에서는 훈련데이타를 신경망에 인가하고 출력단에서의 에러와 cost function 을 구한다.</p>
</li>
<li><p>신경망이 충분히 학습이 되지 못한 경우는 오차가 클 것이며, 이 큰 오차값을 backpropagation 시키면서 가중치와 바이어스 값을 갱신한다.</p>
</li>
<li><p>훈련 데이터에 대해서 반복적으로 이 과정을 거치게 되면, 가중치와 바이어스는 훈련데이타에 최적화 된 값으로 바뀌게 된다.</p>
</li>
<li><p>좋은 학습 결과를 얻으려면, 이 훈련 데이터가 한쪽으로 치우치지 않고 범용성을 가져야 한다는 것은 따로 설명하지 않아도 이해가 될 것 같다.</p>
</li>
<li><p>아래 그림은 feed forward 과정을 거쳐서 이미 에러(δ)를 구하고, 역전파를 통해 반대 방향으로 에러를 전파시키는 과정을 보여주기 위한 것이다.</p>
</li>
<li><p>위 그림에서처럼, 에러(δ)를 구하고, 얻어진 에러를 이용해 (δ4)를 구하여, 비슷한 방식으로 (δ5)를 구할 수 있다. 이렇게 얻어진 정보를 이용해 아래 그림처럼 (δ1)을 구할 수 있다. 이렇게 반복적으로 에러를 전파시키면서 가중치와 바이어스 값을 갱신한다.</p>
</li>
</ul>
<doc-anchor-target id="443-sigmoid-함수의-좋은-성질과-delta-rule">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#443-sigmoid-함수의-좋은-성질과-delta-rule">#</doc-anchor-trigger>
        <span>4.4.3. Sigmoid 함수의 좋은 성질과 delta rule</span>
    </h4>
</doc-anchor-target>
<ul>
<li>이전 class에서 설명을 했듯이 Sigmoid 함수는 좋은 성질을 갖고 있다. 앞서 살펴본 장점 이외에도 미분을 하게 되면 매우 편리한 성질이 있음을 할 수 있다.</li>
<li>f(x) = Sigmoid(x)로 정의를 하게 되면, f의 미분, 즉, 를 얻을 수 있다. 이 관계식을 이용하면, back-propagation 식을 좀 더 쉽게 풀어낼 수 있다.</li>
<li>Sigmoid 함수를 미분하면 위와 같은 식을 갖기 때문에 역전파에 사용하는 δ는 δ = (target ? out) out(1 ? out)과 같이 쉽게 계산을 할 수가 있다. 이것을 delta rule 이라고 부른다.</li>
</ul>
<doc-anchor-target id="444-learning-rate-학습-속도-조절-변수">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#444-learning-rate-학습-속도-조절-변수">#</doc-anchor-trigger>
        <span>4.4.4. Learning rate (학습 속도 조절 변수)</span>
    </h4>
</doc-anchor-target>
<ul>
<li>가중치나 바이어스 값을 갱신할 때는 편미분을 통해서 얻어진 값을 곧바로 곱하지 않고, 학습속도를 조절하기 위한 변수 (이것을 learning rate)를 곱해준다.</li>
<li>보통 η를 사용하며,  처럼 사용한다.</li>
</ul>
<doc-anchor-target id="45-backpropagation-case-study-목적">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#45-backpropagation-case-study-목적">#</doc-anchor-trigger>
        <span>4.5. Backpropagation Case Study 목적</span>
    </h3>
</doc-anchor-target>
<ul>
<li>[Part Ⅱ. Neural Networks] 4. Backpropagation [1] 에서 backpropagation(역전파)의 원리 및 개념에 대해서 알아보았다.</li>
<li>역전파에 대해서 기본 개념은 이해를 했을지라도 실제 이것이 어떻게 적용이 되는지 완전하게 이해를 못했을 수도 있다.</li>
<li>이번 class에서는 역전파의 주요 개념인 “feed forward”와 “error backpropagation”이 실제 어떻게 적용이 되는지에 대해 설명할 예정이다. 구체적인 예를 들어 설명하는 것이 좋겠지만, 이미 설명이 잘 된 자료를 활용하는 것도 좋은 방법 중 하나이다.</li>
<li>이번 class에서는 설명이 잘 된 2개의 자료를 소개할 예정이다. 이 2개의 자료는 backpropagation에 대한 수식의 전개나 유도 등 수학적인 부분에 치중하는 대신에 개념을 이해시키는 것을 목적으로 그림을 통해 단계별 과정을 보여주거나, 실제 숫자를 대입하여 backpropagation의 2 단계가 어떻게 적용이 되는지를 보여준다.</li>
</ul>
<doc-anchor-target id="451-case-study-1">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#451-case-study-1">#</doc-anchor-trigger>
        <span>4.5.1. Case Study 1.</span>
    </h4>
</doc-anchor-target>
<ul>
<li><p>먼저 소개할 자료는 아래 그림처럼 간단한 신경망 회로에 대해서 backpropagation이 진행되는 과정을 단계별로 그림으로 나타내고, 거기에 친절한 주석을 단 자료이며, 그 사이트에 대한 링크는 <a href="http://galaxy.agh.edu.pl/%7Evlsi/AI/backp_t_en/backprop.html">다음</a>과 같다.</p>
</li>
<li><p>입력 x1과 x2가 출력 y로 나오기까지 각각의 넷에 있는 가중치를 이용해서 중간 hidden neuron의 값을 풀어내고, 그 값들을 이용해 최종 y 값을 결정한다. 이렇게 결정된 y는 원래의 target 값과 비교하여 오차(error)를 구하고, 그렇게 구한 오차값을 역방향으로 전파를 시키면서, 오차가 최소화되도록 가중치 값들을 갱신한다. 이 자료는 어려운 수식을 사용하지 않고도 그림만으로 흐름을 설명하는 훌륭한 보조 자료인 것 같다.</p>
</li>
</ul>
<doc-anchor-target id="452-case-study-2">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#452-case-study-2">#</doc-anchor-trigger>
        <span>4.5.2. Case Study 2.</span>
    </h4>
</doc-anchor-target>
<ul>
<li><p>위 자료까지 보더라도 잘 이해가 가지 않는다면, “Matt Mazur”의 블로그 자료가 튜토리얼이나 실 예제 관점에서 가장 설명이 잘된 것 같다. 아래는 <a href="http://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">그의 블로그에 대한 링크</a>이다.</p>
</li>
<li><p>위 그림은 Matt Mazur의 글에서 실제 예를 들 때 사용한 간단한 신경망 회로이다.</p>
<ul>
<li>빨간색은 가중치나 바이어스의 초기값을 나타내며, 파란색은 training data(훈련 데이터)이다.</li>
<li>즉, 입력으로 i1에 0.05를 넣고, i2에는 0.10을 넣은 후에 o1과 o2에서 각각 0.01과 0.99를 나오는지 실제 계산을 해본다.</li>
<li>이 예제에서는 뉴론의 활성화 함수로 sigmoid 함수를 사용했으며, 앞서 그림을 이용한 자료에서도 보았듯이 순차적으로 계산을 한다. 실제 계산을 수행했더니 o1은 약 0.75가 나왔고 o2는 약 0.77이 나왔기 때문에 상당한 오차가 발생했음을 알 수 있다. 이 오차를 다시 반대 방향으로 전파를 시키면서 w1 ~ w6의 값을 갱신한다. 이 때 학습의 속도를 고려하는 학습 진도율(η)이 등장하며, 이 예제에서는 0.5를 사용하였다.</li>
</ul>
</li>
<li><p>친절하게도 source code까지 공개를 하였으니, 이 값을 바꿔보면 학습의 속도가 달라짐을 확인할 수 있을 것이다.</p>
<ul>
<li>이 방식으로 w1 ~ w6의 값을 모두 갱신을 하였다면, 다시 훈련 데이터를 i1과 i2에 넣고 위 과정을 반복한다. 반복하다 보면, o1과 o2가 원래의 학습 목표였던 0.01과 0.99에 근사한 값이 도달 될 수 있을 것이다</li>
<li>이 때의 w1 ~ w6 는 훈련 데이터를 통해 최적화 된, 즉 학습이 된, 상태가 되는 것이다. 이 예제는 총 10000회를 반복적으로 수행하면, 원하는 값에 도달하게 된다. 훈련 데이터가 많아지고 신경망에 있는 뉴런의 숫자가 많아질수록 학습 시간이 길어지고, 초기값이나 학습 진도율 변수가 학습에 상당한 영향을 끼칠 것이라는 것에 대한 감을 잡게 될 것으로 확신한다.
위 2개의 링크에 있는 자료는 Google 검색을 통해, 필자가 찾은 예 중에서 가장 잘된 자료 같다.</li>
</ul>
</li>
<li><p>역전파에 대한 이해에 도움이 되길 기대하며, 다음 class에서는 Overfitting(과학습, Overtraining)이라는 개념에 대해 살펴볼 예정이다.</p>
</li>
</ul>
<doc-anchor-target id="마무리">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#마무리">#</doc-anchor-trigger>
        <span>마무리</span>
    </h2>
</doc-anchor-target>
<ul>
<li>Backpropagation은 처음 접했을 때, 기본 개념은 어느 정도 이해가 가더라도 아마 알쏭달쏭한 부분이 있을 것이다.</li>
<li>다음 class에서는 간단한 신경망 회로에서 backpropagation이 어떻게 적용이 되는지 실제 예를 들어 설명할 예정이다. 이것들을 보면, 이번 class에서 100% 이해를 못하더라도 다음 class에서는 100%가 될 것이라고 확신한다.</li>
</ul>
<doc-anchor-target id="요약-정보">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#요약-정보">#</doc-anchor-trigger>
        <span>요약 정보</span>
    </h2>
</doc-anchor-target>
<ol>
<li><p>Neural Network ? Backpropagation은?</p>
<ul>
<li>최적의 학습 결과를 찾기 위해 역방향으로 에러를 전파(backward propagation of error)</li>
</ul>
</li>
<li><p>Backpropagation 이해를 위한 Cost function 개념</p>
<ul>
<li>입력된 훈련 데이터에 관한 실제 출력과 기대 출력간의 차이</li>
<li>cost function이 최소값이 되도록(실제 출력과 기대 출력의 차이가 없도록) Backpropagation</li>
</ul>
</li>
<li><p>Backpropagation 이해를 위한 Gradient-descent 개념</p>
<ul>
<li>cost function이 최소가 되도록 하는 방법</li>
<li>cost function의 인자인 가중치(w)와 바이어스(b)를 조절함</li>
</ul>
</li>
<li><p>Backpropagation의 두 단계</p>
<ul>
<li>feed forward: 입력 -&gt; 출력(최종 출력단에서 에러와 cost function을 구함)</li>
<li>backpropagation: 출력부터 반대 방향으로 순차적으로 편미분을 수행해가면서 뉴런의 가중치(w)와 바이어스(b) 값을 갱신</li>
</ul>
</li>
<li><p>Backpropagation에서 Sigmoid 함수를 사용하는 이유</p>
<ul>
<li>Backpropagation을 해석하기에 편리한 Sigmoid 함수의 미분 성질</li>
</ul>
</li>
</ol>


<div class="flex items-center mt-6 mb-6">
    <span>
        <a href="/tech/tags/">
            <svg class="inline-block -mb-px mr-1.5 text-blue-500 dark:text-blue-400" xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" overflow="visible" fill="currentColor"><g><path d="M21.3 9.88l-8.59-8.59C12.52 1.11 12.27 1 12 1H2c-.55 0-1 .45-1 1v10c0 .27.11.52.29.71l8.59 8.58c.57.57 1.32.88 2.12.88s1.55-.31 2.12-.88l7.17-7.17a3.009 3.009 0 00.01-4.24zm-1.42 2.82l-7.17 7.17c-.39.39-1.02.39-1.42 0L3 11.59V3h8.59l8.29 8.29c.39.39.39 1.03 0 1.41z" /><path d="M7.01 6C6.45 6 6 6.45 6 7s.45 1 1 1 1-.45 1-1-.44-1-.99-1z" /></g></svg>
        </a>
        <span class="mr-2"><a href="/tech/tags/ml/" class="no-link inline-flex items-center justify-center font-medium leading-none whitespace-nowrap text-blue-500 dark:text-blue-400 border border-blue-500 dark:border-blue-400 hover:bg-blue-100 dark:hover:bg-transparent dark:hover:border-blue-200 dark:hover:text-blue-200 transition-colors duration-200 ease-out h-6 px-2 text-xs rounded-md">
    <span>ml</span>
</a></span>
        <span class="mr-2"><a href="/tech/tags/coursework/" class="no-link inline-flex items-center justify-center font-medium leading-none whitespace-nowrap text-blue-500 dark:text-blue-400 border border-blue-500 dark:border-blue-400 hover:bg-blue-100 dark:hover:bg-transparent dark:hover:border-blue-200 dark:hover:text-blue-200 transition-colors duration-200 ease-out h-6 px-2 text-xs rounded-md">
    <span>coursework</span>
</a></span>
    </span>
</div>


                <!-- Required only on API pages -->
                <doc-toolbar-member-filter-no-results />
            </div>

            
<nav class="flex mt-14">
    <div class="w-1/2">
        <a class="px-5 py-4 h-full flex items-center break-all md:break-normal font-medium text-blue-500 dark:text-blue-400 border border-gray-300 hover:border-gray-400 dark:border-dark-650 dark:hover:border-dark-450 rounded-l-lg transition-colors duration-150 relative hover:z-5" href="/tech/1-ml-intro/">
            <svg xmlns="http://www.w3.org/2000/svg" class="mr-3" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M19 11H7.41l5.29-5.29a.996.996 0 10-1.41-1.41l-7 7a1 1 0 000 1.42l7 7a1.024 1.024 0 001.42-.01.996.996 0 000-1.41L7.41 13H19c.55 0 1-.45 1-1s-.45-1-1-1z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
            <span>
                <span class="block text-xs font-normal text-gray-400 dark:text-dark-400">Previous</span>
                <span class="block mt-1">#ML 1. 기계학습</span>
            </span>
        </a>
    </div>

    <div class="w-1/2">
        <a class="px-5 py-4 -mx-px h-full flex items-center justify-end break-all md:break-normal font-medium text-blue-500 dark:text-blue-400 border border-gray-300 hover:border-gray-400 dark:border-dark-650 dark:hover:border-dark-450 rounded-r-lg transition-colors duration-150 relative hover:z-5" href="/tech/faq/">
            <span>
                <span class="block text-xs font-normal text-right text-gray-400 dark:text-dark-400">Next</span>
                <span class="block mt-1">Contact</span>
            </span>
            <svg xmlns="http://www.w3.org/2000/svg" class="ml-3" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M19.92 12.38a1 1 0 00-.22-1.09l-7-7a.996.996 0 10-1.41 1.41l5.3 5.3H5c-.55 0-1 .45-1 1s.45 1 1 1h11.59l-5.29 5.29a.996.996 0 000 1.41c.19.2.44.3.7.3s.51-.1.71-.29l7-7c.09-.09.16-.21.21-.33z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
        </a>
    </div>
</nav>


        </main>

        <div class="border-t dark:border-dark-650 pt-6 mb-8">
            <footer class="flex flex-wrap items-center justify-between">
    <div>
        <ul class="flex flex-wrap items-center text-sm">
</ul>

    </div>
    <div class="docs-copyright py-2 text-gray-500 dark:text-dark-350 text-sm leading-relaxed"><p>© Copyright 2020 - 2021 KyuHwan Shim, All rights reserved.</p>
</div>
</footer>

        </div>
    </div>
    
    <!-- Rendered if sidebar right is enabled -->
    <!-- Sidebar right skeleton-->
    <div v-cloak class="fixed top-0 bottom-0 right-0 transform translate-x-full bg-white border-gray-200 lg:sticky lg:border-l lg:flex-shrink-0 lg:pt-6 lg:transform-none lg:w-56 lg:z-0 md:w-72 sidebar-right skeleton dark:bg-dark-850 dark:border-dark-650">
        <div class="pl-5">
            <div class="w-32 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
            <div class="w-48 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
            <div class="w-40 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
        </div>
    </div>
    
    <!-- User should be able to hide sidebar right -->
    <doc-sidebar-right v-cloak></doc-sidebar-right>
</div>

        </div>
    </div>

    <doc-search-mobile></doc-search-mobile>
    <doc-back-to-top></doc-back-to-top>
</div>


        <div id="docs-overlay-target"></div>

        <script>window.__DOCS__ = { "title": "#ML 2.Neural Network", icon: "file", hasPrism: false, hasMermaid: false }</script>
    </body>
</html>
